---
title: "Nesta attacks study"
author: "Patrycja Tempska and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: yes
    df_print: kable
    keep_tex: yes
    includes:
      in_header: Rafal_latex6.sty
  html_document:
    df_print: paged
  word_document: default
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: ../references/attacks.bib
csl: ../references/apa-6th-edition.csl
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = '../')

#libraries used
library(ggplot2)
library(ggthemes)
library(gridExtra)

```



\tableofcontents

# Section Abstract
This article describes an experimental intervention study conducted in a naturalistic, digital setting (Q&A forum on Reddit), utilizing a collective intelligence approach to content moderation and reduction of the level of verbal aggression among a selected group of Reddit users who regularly attack other community members. Collective Intelligence in this sense means exploring the collaboration between human and machine intelligence to develop solutions to social challenges. Artificial Intelligence was used to detect verbal aggression (personal attacks) and notify human volunteers about attacks. Volunteers after receiving notifications employed interventions based on norm or empathy promotion. We find that only those who were sanctioned with norms-inducing interventions had their personal attacks' level significantly decreased. 

\vspace{1mm}
\footnotesize
```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
2+2 #use this formatting for chunks
```
\normalsize


# Introduction
Although much effort has been made in order to tackle the problem of verbal aggression and harassment online, looking at various reports and surveys [@sorrentino2019epidemiology; @vogels_state_2021; @Zachary_hate], it remains a common hindrance for people engaging with social media in their everyday lives. The problem got exacerbated in the midst of the COVID19 pandemic, during which the majority of our social life moved to cyberspace. During this shift, there was an increase in cyberbullying attitudes and perpetration [@barlett2021comparing], 90% increase in public reports of illegal online content [@grant_2021], including 114% increase in non-consensual sharing of intimate images, 30% increase in cyberbullying, as well as 40% of increase in adults reporting online harassment. According to a report conducted by company L1ght [@noauthor_l1ght_2020], hate speech directed towards China and the Chinese went up by 900% on Twitter. Gaming platforms were in the spotlight as well, with a 40% increase in toxicity on Discord \footnote{In both reports, the increase is reported as a relative change between the year 2019 and 2020, with no absolute indicators.}.


But alongside the growing need for even more efficient and proactive moderation, the capacity to execute it did not go hand in hand, forcing companies and policymakers to rethink the current model of moderation processes and workforce. Due to the COVID19 restrictions including social distancing, a lot of those serving the role of moderators had to be sent home [@bhattacharya] without the ability to work remotely because of the constraints affiliated with restrictive non-disclosure agreements (NDA) among others. Curtailing the moderators' workforce was accompanied by more agency given to algorithms and AI-based moderation. Those changes, as argued by @gerrard2020covid19, can be seen as a serious red flag in terms of safety for all users on online platforms. 

# Automated versus Human-based Moderation
The hindrances and threats that go along with the Artificial Intelligence-based methods for moderation have been widely debated, with the most critical discussions revolving around technology performance [@macavaney2019hate; @schmidt2017survey]. State-of-the-art solutions are mostly governed by statistical methods including deep learning and machine learning [@lecun2015deep; @sejnowski2020unreasonable; @jordan2015machine]. Their performance is inherently tied to the amount of data being fed to the system and the quality of its annotation. 
\todo{This deserves a developed paragraph with examples}At different stages of the process, from data gathering and preparation, annotation to the training or algorithms themselves, biases seem to be omnipresent [@binns2017like, @geva2019we @mehrabi2021survey].

\todo{This deserves a developed paragraph with examples}Users' of online services are also creative in their strategies to circumvent automated content moderation systems and as shown by @grondahl2018all, current techniques are vulnerable to the most common evasion attacks like word changes (inseting typos and leetspeak), word-boundary changes (inserting or removing whitespace), or word appending (appending common or non-hateful words like "love"). 

\todo{This deserves a developed paragraph with examples}Lack of generalisability of the models---the ability to perform well on datasets coming from sources other than the one used for training---is a serious shortcoming as well [@yin2021towards; @swamy2019studying; @rosa2019automatic]. 

As shown by @wu2019errudite, @lipton2019troubling, and @musgrave2020metric in practice, the development of such models often lacks thorough error analysis and legitimate experimental methodology, which can result in non-reproducibility. This is also connected with a potential lack of thorough understanding of the limitations of the models and spurious conclusions being announced to a wider public. Specifically, @lipton2019troubling distinguishes four dysfunctional patterns occurring in the current research paradigm in the industry and academia alike. 

\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
\item First, the inability to draw a clear distinction between speculation and explanation, with the first one often being disguised as the second. For instance, in a paper from 2017 [@steinhardt2017certified], Jacob Steinhardt, the author of @lipton2019troubling, admitted to stating that “the high dimensionality and abundance of irrelevant features... give the attacker more room to construct attacks” - although no experiments were conducted to measure what is the effect of dimensionality of the neural network on its attackability. 
\item Second, the inability of successful identification of the sources of performance improvement (whether it was problem formulation, optimization of the heuristics, data-preprocessing, hyperparameter tuning, or perhaps yet another aspect). As was shown by Gabor Melis, Chris Dyer, and Phil Blunsom, some improvements in language modeling which originally were ascribed to complex innovations in the architecture of the network, stem from hyperparameter tuning [@melis2017state]. As mentioned by @lipton2019troubling, there is a tremendous value coming from the thorough understanding of a particular method, and a variety of techniques are vital in the process (like ablation, robustness checks, qualitative error analysis) for the benefit of the whole community.
\item Third, "mathiness"---the use of obscure language and often covering weak argumentation with the alluring but often apparent depth of technical jargon. Again Jacob Steinhardt admitted infusing his 2015 paper co-authored with Percy Liang [@steinhardt2015learning] with an irrelevant theorem to amplify the empirical results. They discussed “staged strong Doeblin chains” which actually had limited pertinence to the learning algorithm---the main subject of a paper. 
\item Last but not least---misuse of language\todo{Give an example}. This includes suggestive definitions without proper explanation of what they mean in the context (e.g. inflating good performance in simple NLP tasks to human-level understanding), overloading the papers with technical terminology, or suitcase words (words that can encompass a variety of meanings, e.g. consciousness) 
 \end{enumerate}

Yet another obstacle in the process is the lack of gold standard in dataset creation and taxonomies of abusive language being used for instance in the process of annotating different datasets. Frequently people obtain data from various sources and do not follow any universally used instructions when it comes to annotation, leading to discrepancies between various datasets being tagged within one domain (e.g. hate speech). Lack of expert annotators and proper annotation criteria and instructions are also widespread, with the common practice of hiring untrained workers from Mechanical Turk or other crowdsourcing platforms.

Although there are some initiatives developed in response, most notably, functional tests for Hate Speech Detection Models created by @rottger2020hatecheck, or the Online Safety Data Initiative (OSDI) [@onlinesafetydata], focused on projects related to improving access to data, standardizing the description of online harms, as well as creating tools and benchmarks for evaluation of technologies focused on safety, much effort must be made before wider adoption of such solutions comes into force.  

At the same time, only automated methods can scan through the massive amount of content being generated every day on different platforms. On Facebook, there are more than 3B comments and likes daily [@noauthor_facebook:_2012], 500M tweets are sent daily on Twitter [@noauthor_10_2021], and over 2B comments made by users of Reddit in 2020 [@noauthor_reddit_nodate] which is almost 3M comments made daily. With this amount of content, it's either impossible or extremely costly to scale the moderation workforce. One can also have doubts about the ethical aspects of hiring workers who are often unaware of how this kind of task will affect their well-being. Being submersed in the cyber-Augean stables takes a toll on many---as examined by @roberts2014behind & @roberts2016commercial. Workers hired for such tasks are often low-status and low-wage, isolated and asked to keep what they've seen in secret under restrictive NDAs. This in turn makes the research in the area extremely difficult, since moderators are not allowed to talk about their work conditions or any other related subject. Those who decided to break the NDA are risking a penalty. 
Screening through the reported user-generated content is connected with exposure to violent and deeply disturbing materials, with child pornography, murders, or suicides as examples of the most extreme cases. This can lead to serious psychological damage, such as depression, or PTSD [@roberts2014behind]. Although there are certain initiatives being developed or introduced to reduce the emotional impact of the moderation, like stylistic alterations to a content (applying grayscale or blurring to images) [@karunakaran2019testing], workplace wellness programs, clinical support, or psychological training [@steiger2021psychological], none of the methods can eliminate the psychological distress completely. Some of the employees filed a lawsuit against Facebook and as a result, the company agreed to pay $52M in compensation for mental health issues developed during the job [@newton_facebook_2020]. Also as described by @parks2019dirty, the work is often performed under time pressure, reviewing 25K pieces of content per day. Spending on average three to five seconds on each image reported for moderation might not lead to the most thoughtful decisions and as shown @stepanikova2012racial, high time pressure can amplify human biases.
Taking into consideration that Facebook employs 15K moderators [@koetsier_report] and most likely more are needed to keep up with the growing amount of content, with the parallel considerations about the negative effects of content moderation on mental health, a collaboration between humans and machines in this area seems inevitable. 

# Pro-active and reactive moderation
There are different approaches when it comes to the moderation of online content. One can follow the workflow of reactive moderation, which happens once the content is published. Harmful messages can be either reported by the users of the platforms or automated methods and then sent for review to moderators. A set of actions can be then taken depending on the platform and their community guidelines---on the content or user level. A harmful message can be deleted, made invisible to other users, or certain profanities can be altered with special signs to censor them. Depending on the type and amount of infraction, a particular user can be warned, muted, shadowbanned, or banned from further participation in the community for a period of time. The weakness of the reactive method is that the damage is done. Whoever is the recipient of the abusive message has the chance to see it and potentially suffer [@hoff2009cyberbullying, @keipi2016online, @wachs2019associations]. 
Yet another weakness connected with relying solely on human reports is the content that is harmful but unreported by a receipted or any bystander. Although the exact scale of unreported content is not known, various self-report studies show that a lot of children, teens, or even adults do not report cyberbullying or harassment online [@noauthor_:game_nodate; @noauthor_free_nodate; @french_as_2021].

Yet another type of moderation can be distinguished as pro-active or pre-moderation. In pre-moderation, automated methods are either based on Artificial Intelligence or other less sophisticated tools (e.g. blacklists) and can screen the content before it gets published. If a type of harmful message gets detected, it can be removed before reaching the recipient. Due to the aforementioned dubious performance of state-of-the-art statistical methods, particularly low precision, they are rarely used autonomously. 

Pro-active moderation can be utilized using AI or other methods to promote socially positive engagement. Instead of or in the combination with punitive solutions such as privileges restriction, one can induce empathy or community norms with counter-speech. Counter speech as described by Dangerous Speech Project [@noauthor_counterspeech_2017] is "any direct response to hateful or harmful speech which seeks to undermine it". As examined by @munger2017tweetment, counter-speech can be effective in the reduction of racist tweets (although only in the condition in which a white male with high followers was approaching another white male). In a study conducted by @bilewicz2021artificial, a bot disguised as a Reddit user, equipped with normative and empathetic interventions, significantly decreased the amount of personal attacks generated on Reddit. In yet another study by @mivskolci2020countering, this technique was not effective in changing the behavior of the users (counter-speech here aimed at reducing the prejudice against Roma minority in Slovakia), but encouraged bystanders to express pro-Roma comments on specific Facebook posts. Counter-speech also has been shown to have the potential to increase civility online in studies conducted by @friess2021collective, @molina2018role, @han2018civility. 
...


# Collective Intelligence Approach to Counter-speech
Traditionally collective intelligence has been defined as "a group or a team’s combined capacity and capability to perform a wide variety of tasks and solve diverse problems" [@noauthor_collective_nodate]. In our paper and in the theoretical underpinnings of the experiment itself, we will be relying on a collective intelligence scope proposed by Nesta, an innovation foundation (https://www.nesta.org.uk), which focuses on a collaboration between human and machine intelligence to develop innovative solutions to social challenges. (Gdzieś stopka(?): Samurai Labs has been one of the 15 recipients of the second round of collective intelligence grants awarded by Nesta: https://www.nesta.org.uk/project-updates/second-round-collective-intelligence-grants/) 

The main objective of the experiment was to test whether the level of verbal aggression (personal attacks) of a group of users' regularly attacking others on Reddit can be significantly decreased by community-driven, counter-speech interventions conducted by volunteers in partnership with Artificial Intelligence. Instead of using negative motivation system, the assumption was to test a positive one - convincing verbally violent users to refrain from using cyberviolence based on peer-pressure regulation and experiential learning of a positive set of norms and empathy. Algorithms developed for the detection of personal attacks were used to monitor the activity of experimental groups and notify volunteers about all attacks generated by its' members. Volunteers, after receiving a notification on Slack, could then react with a proper intervention. Such an approach served as a distributed bottom-up voluntary model of moderation based on collective intelligence---utilizing human + machine intelligence. 

In the end, what we were able to compare was the following: the effectiveness of the existing Reddit moderation system (predominantly grounded in a punitive authoritarian paradigm) versus the existing moderation system combined with collective intelligence---Artificial Intelligence supported with a crowd of volunteers---who introduced the element of  positive peer-pressure. 

Normative interventions can refer either to general social norms of civility and respect, community standards, a particular subreddit rule, or a descriptive norm among others.
E.g. Insulting others is against Reddit's policy. 
Hey there, we do not call each other like that here.
Empathetic interventions can refer to the emotional state of the recipient or the sender of the attack or both. They are designed to evoke an empathetic response. 
E.g. There is a human being on the other side who might be hurt by your words.
I see you are frustrated but remember the human.

Cyberviolence was defined in this experiment as a personal attack - any kind of verbal harassment, insult, or threat directed against the interlocutor in a text-based conversation online. Those were detected using Samurai Labs' cyberviolence detection system. 

The following hypotheses were formulated: 

H1: If a group of human volunteers notified by an AI-based cyberviolence detection system about cyberviolence generated by the treatment group users (cyberviolence will be defined as a personal attack, harassment, or a threat targeted against an interlocutor) responds with counter-speech interventions, this will result in a decreased cyberviolence level for the whole group after the intervention period. 

H2: If two groups receive different types of interventions (empathy-based or normative), then the decrease in cyberviolence will be larger in the case of normative interventions in comparison to the empathy-based ones.

# Experimental Design and data collection
This was a 6-months field experiment in a digital setting conducted on a popular Q&A and news forum, Reddit (www.reddit.com). 
We formed treatment and control groups based on three main criteria:
\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
 \item During the intervention period, we have expected to have 20 active volunteers at any given time, each willing to conduct 10 interventions daily. Thus, we needed approximately 200 attacks daily generated by the treatment groups.
\item The identification of users who regularly attack others was necessary to measure the effect of interventions at the end.
\item The identification of users who were active during the whole preliminary monitoring period was necessary to minimize the risk of attrition during the study. 
 \end{enumerate}


User identification process:
\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
   \item First, we obtained 1 week of real-time (coming from 15-22nd of February 2020), unmoderated data from Reddit. The content was downloaded from the data stream provided by pushshift.io. 
   \item Samurai Labs Artificial Intelligence for personal attacks detection was applied to identify users who attacked others at least once within the aforementioned timeframe. This resulted in the identification of 93966 users. 
   \item We removed all accounts which we suspected not to be run by humans (AutoModerator and all users which had "bot" in the username string). This resulted in the removal of 388 users, thus 93578 were left on our list.
   \item Next, we removed users who generated only 1 personal attack during the week (leaving only those who attacked at least twice). As mentioned, the group of those regularly attacking others was crucial to measure the effects of the interventions. This step resulted in the removal of users below the third quartile (Q3). 20124 users were left in our group.
   \item Moving forward, we removed users who generated less than 14 comments in this week. We cared about most active users, and 2 comments per day per person on average seemed reasonable (not sure yet how to justify this - 14 comments is below 1st quartile (Q1:28, Q2: 63, Q3:126, mean=103)). This resulted in the removal of 2192 users, so 17932 were left.
   \item We discarded users whose personal attacks to all comments ratio was below 2\%. This means the inclusion in the sample of users above the 1st quartile. 4422 users were removed, leaving us with a group of 13510. 
   \item The next step of the process begun on March 9th, 2020, and lasted until May 5th, 2020 (9 weeks). During this period we have monitored the activity of the identified group of 13510 users and applied further selection criteria to make sure we select those who were regularly active and attacked other users. 
   \item The period of monitoring was divided into weeks. We have discarded those weeks during which technical difficulties occurred with the pushshift.io (resulting in missing data). Thus, we have taken into consideration only 6 full weeks for the period.
   \item Users who generated at least 1 attack during 5 out of 6 weeks were identified. First, we planned to restrict the list to only those users, who generate at least 1 attack during each week (6/6) but such restrictive criterion led to only 255 users left, which was not enough for the study. The less restrictive criterion (at least 1 attack generated during 5/6 weeks) resulted in 694 people. 
   \item Next, we calculated the daily average number of personal attacks generated by the group who met the above criteria (which resulted in 357 attacks per day, 1.94 attacks daily per person on average).
   \item Knowing that we need around 200 attacks/daily per treatment group (just enough for volunteers to keep up according to our assumption), we have randomly selected 195 users per each treatment group (normative and empathetic). The rest was delegated as a control group (304 users). 
   
 \end{enumerate}


The duration of the experiment, 6 months, was divided into three 2-months periods. The first two months served as a monitoring period to properly select groups and establish baselines. The next 2 months served as treatment period, during which groups received counter-speech comments from volunteers, in response to personal attacks detected by the Artificial Intelligence-based system. The last 2-months served as the post-treatment monitoring period to gather the data needed to evaluate the effectiveness of interventions. 



# Results





#  References {-}

\vspace{-3mm}





