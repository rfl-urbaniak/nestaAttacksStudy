% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  10pt,
  dvipsnames,enabledeprecatedfontcommands]{scrartcl}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Investigating the impact of interventionist causal approach on the study of verbal aggression online},
  pdfauthor={Patrycja Tempska and Rafal Urbaniak},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\documentclass{article}

% %packages
 \usepackage{booktabs}

\usepackage{multirow}

\usepackage{graphicx}
\usepackage{longtable}
\usepackage{ragged2e}
\usepackage{etex}
%\usepackage{yfonts}
\usepackage{marvosym}
\usepackage[notextcomp]{kpfonts}
\usepackage{nicefrac}
\newcommand*{\QED}{\hfill \footnotesize {\sc Q.e.d.}}
\usepackage{floatrow}

\usepackage[textsize=footnotesize]{todonotes}
%\linespread{1.5}


\setlength{\parindent}{10pt}
\setlength{\parskip}{1pt}


%language
\usepackage{times}
\usepackage{t1enc}
%\usepackage[utf8x]{inputenc}
%\usepackage[polish]{babel}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx}
\usepackage[scaled=0.88]{helvet}


%AMS
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{geometry}
 \geometry{a4paper,left=35mm,top=20mm,}


%environments
\newtheorem{fact}{Fact}



%abbreviations
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\n}{\neg}
\newcommand{\et}{\wedge}
\newcommand{\jt}{\rightarrow}
\newcommand{\ko}[1]{\forall  #1\,}
\newcommand{\ro}{\leftrightarrow}
\newcommand{\exi}[1]{\exists\, {_{#1}}}
\newcommand{\pr}[1]{\mathsf{P}(#1)}
\newcommand{\cost}{\mathsf{cost}}


\newcommand{\odds}{\mathsf{Odds}}
\newcommand{\ind}{\mathsf{Ind}}
\newcommand{\nf}[2]{\nicefrac{#1\,}{#2}}
\newcommand{\R}[1]{\texttt{#1}}
\newcommand{\prr}[1]{\mbox{$\mathtt{P}_{prior}(#1)$}}
\newcommand{\prp}[1]{\mbox{$\mathtt{P}_{posterior}(#1)$}}



\newtheorem{q}{\color{blue}Question}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}



%technical intermezzo
%---------------------

\newcommand{\intermezzoa}{
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}



	\tiny{\sc Optional Content Starts}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}\nopagebreak 
	}


\newcommand{\intermezzob}{\nopagebreak 
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}

	\tiny{\sc Optional Content Ends}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}
	}
%--------------------






















\newtheorem*{reply*}{Reply}
\usepackage{enumitem}
\newcommand{\question}[1]{\begin{enumerate}[resume,leftmargin=0cm,labelsep=0cm,align=left]
\item #1
\end{enumerate}}

\usepackage{float}

% \setbeamertemplate{blocks}[rounded][shadow=true]
% \setbeamertemplate{itemize items}[ball]
% \AtBeginPart{}
% \AtBeginSection{}
% \AtBeginSubsection{}
% \AtBeginSubsubsection{}
% \setlength{\emergencystretch}{0em}
% \setlength{\parskip}{0pt}






\usepackage[authoryear]{natbib}

%\bibliographystyle{apalike}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Investigating the impact of interventionist causal approach on
the study of verbal aggression online}
\author{Patrycja Tempska and Rafal Urbaniak}
\date{}

\begin{document}
\maketitle

\tableofcontents

\vspace{1mm}
\footnotesize

\normalsize

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

We live in a world in which there is an abundance of data. Referring the
phrase coined by Clive Humby, ``data is the new oil'' --- data has
displaced oil and become the world's most valuable resource. Albeit
there are some fundamental differences between the two assets. Oil
belongs to a very limited and finite resource, while data is growing at
an exponential rate (cite:
\url{https://cloudtweaks.com/2015/03/how-much-data-is-produced-every-day/}).
The value of oil does not depend on its amount or the refinery processes
- it's rather constant at a particular dynamic of market's supply and
demand. Data and its value, on the contrary, is inherently tied to its
amount and the ``refinery process'' - various tools and techniques for
its discovery and analysis. In some cases, certain techniques can be
used only if one has a sufficient (meaning enormous) data at one's
disposal --- e.g.~statistical methods like machine learning and deep
learning \todo{@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}}. In the last couple of years, more and more industry and academia
representatives alike started to research and develop tools for causal
discovery and inference. Unfortunately, although thinking in causal
terms seems to be our inherent human capacity and one of our basic
cognitive abilities, that is not necessarily the case with the majority
of toolkits and statistical models developed to draw insight and
correlations from data. In classical statistics, which is stripped of
causality-related terms and reasoning, one cannot speak about causal
effects, unless one deals with a randomized controlled experiment. This
comprises one of the limits of the classical paradigm in statistics
(among others such as arbitrary thresholds of statistical significance,
p-hacking, disregard of prior information, and others which we will not
discuss here). But often scientists conduct the so-called
cross-sectional research, in which no variable is manipulated. In the
language and toolkit of classical statistics in such cases we cannot
conduct any causal inference. What we can measure using those methods is
correlation, and, as frequently repeated, ``correlation does not imply
causation.'' Correlation can be positive, negative, or there can be no
correlation at all. A positive one implies that the increase of the
value of one variable will influence the increase of another variables'
value. A negative correlation means that the change in values of two
variables will diverge into separate directions --- one will increase
and one will decrease alongside the first one. We can measure the
strength of the correlation using various mathematical methods, where 1
or -1 means we are dealing with perfect correlation --- the value of one
variable changes proportional to the value of another variable. But even
though causation is necessarily connected with correlation, as already
mentioned --- not every correlation implies causation. This is
especially problematic if we take into consideration the fact that
statisticians and people in general naturally think in causal terms and
spurious interpretations of studies occur frequently. On the other hand,
a conservative approach of many classical statisticians led to a
long-living debate on whether smoking causes cancer, lasting for over 20
years. One of the reasons for such a fierce debate was the lack of a
causal framework, which was still in its infancy at that time. Framework
of tools with which we could validate or invalidate assumptions about
the existence of the potential confounder --- a variable causing both
craving cigarettes and lung cancer (e.g.~some genetic factor), creating
a spurious correlation between cigarettes and lung cancer. Modeling
causality and moving beyond traditional statistics is a relatively young
endeavor. One of the pioneers of the work on causality is Judea Pearl.
As he describes, causality and traditional statistics are governed by
opposing ways of thinking. While statistics might be described as
``study of methods of the reduction of data,'' causality requires the
incorporation of an extra layer --- the understanding or prior knowledge
(assumptions) about the process that generate the data in the first
place. Classical statistics might be described here as a ``model-free
approach,'' just as Artificial Intelligence methods based on machine
learning/deep learning, in which no prior knowledge can be taken into
account.

In this paper, I will show how mathematical methods for process modeling
which includes assumptions about causal connections can be applied to
discover how different variables affect each other, moving beyond
classical statistics. Looking at the analysis from two perspectives ---
traditional (classical, frequentist statistics) and bayesian -- we will
see the advantage of the second paradigm in learning about the world
based on the data we have at our disposal. Data used for analysis
concerned in this paper comes from an experimental intervention study
conducted in a naturalistic, digital setting (Q\&A forum on Reddit),
utilizing a collective intelligence approach to content moderation and
reduction of the level of verbal aggression among a selected group of
Reddit users who regularly attack other community members. Collective
Intelligence in this sense means exploring the collaboration between
human and machine intelligence to develop solutions to social
challenges. Artificial Intelligence was used to detect verbal aggression
(personal attacks) and notify human volunteers about attacks. Volunteers
after receiving notifications employed interventions inspired in its
underlying principles in philosophy and psychology. There were two broad
categories of interventions - one based on norms (deontology, virtue
ethics among others) and the second based on empathy (inspired by
notions of David Hume and Adam Smith).

One might also wonder about the motivations for conducting such an
experiment. Much effort has been made in order to tackle the problem of
verbal aggression and harassment online, but looking at various reports
and surveys (Laub, 2019; Sorrentino, Baldry, Farrington, \& Blaya, 2019;
Vogels, 2021), it remains a common hindrance for people engaging with
social media in their everyday lives. The problem got exacerbated in the
midst of the COVID19 pandemic, during which the majority of our social
life moved to cyberspace. During this shift, there was an increase in
cyberbullying attitudes and perpetration (Barlett, Simmers, Roth, \&
Gentile, 2021), 90\% increase in public reports of illegal online
content (Grant, 2021), including 114\% increase in non-consensual
sharing of intimate images, 30\% increase in cyberbullying, as well as
40\% of increase in adults reporting online harassment. According to a
report conducted by company L1ght (L1ght, 2020), hate speech directed
towards China and the Chinese went up by 900\% on Twitter. Gaming
platforms were in the spotlight as well, with a 40\% increase in
toxicity on Discord
\footnote{In both reports, the increase is reported as a relative change between the year 2019 and 2020, with no absolute indicators.}.

But alongside the growing need for even more efficient and proactive
moderation, the capacity to execute it did not go hand in hand, forcing
companies and policymakers to rethink the current model of moderation
processes and workforce. Due to the COVID19 restrictions including
social distancing, a lot of those serving the role of moderators had to
be sent home (Bhattacharya, 2021) without the ability to work remotely
because of the constraints affiliated with restrictive non-disclosure
agreements (NDA) among others. Curtailing the moderators' workforce was
accompanied by more agency given to algorithms and AI-based moderation.
Those changes, as argued by Gerrard (2020), can be seen as a serious red
flag in terms of safety for all users on online platforms. That is why
in the paper I will also try to sketch a landscape of current moderation
techniques through a critical lens.

\hypertarget{automated-versus-human-based-moderation}{%
\section{Automated versus Human-based
Moderation}\label{automated-versus-human-based-moderation}}

The hindrances and threats that go along with the Artificial
Intelligence-based methods for moderation have been widely debated, with
the most critical discussions revolving around technology performance
(MacAvaney et al., 2019; Schmidt \& Wiegand, 2017). State-of-the-art
solutions are mostly governed by statistical methods including deep
learning and machine learning (Jordan \& Mitchell, 2015; LeCun, Bengio,
\& Hinton, 2015; Sejnowski, 2020). Their performance is inherently tied
to the amount of data being fed to the system and the quality of its
annotation. \todo{This deserves a developed paragraph with examples}At
different stages of the process, from data gathering and preparation,
annotation to the training or algorithms themselves, biases seem to be
omnipresent Mehrabi, Morstatter, Saxena, Lerman, \& Galstyan (2021).

\todo{This deserves a developed paragraph with examples}Users' of online
services are also creative in their strategies to circumvent automated
content moderation systems and as shown by Gröndahl, Pajola, Juuti,
Conti, \& Asokan (2018), current techniques are vulnerable to the most
common evasion attacks like word changes (inseting typos and leetspeak),
word-boundary changes (inserting or removing whitespace), or word
appending (appending common or non-hateful words like ``love'').

\todo{This deserves a developed paragraph with examples}Lack of
generalisability of the models---the ability to perform well on datasets
coming from sources other than the one used for training---is a serious
shortcoming as well (Rosa et al., 2019; Swamy, Jamatia, \& Gambäck,
2019; Yin \& Zubiaga, 2021).

As shown by Wu, Ribeiro, Heer, \& Weld (2019), Lipton \& Steinhardt
(2019), and Musgrave, Belongie, \& Lim (2020) in practice, the
development of such models often lacks thorough error analysis and
legitimate experimental methodology, which can result in
non-reproducibility. This is also connected with a potential lack of
thorough understanding of the limitations of the models and spurious
conclusions being announced to a wider public. Specifically, Lipton \&
Steinhardt (2019) distinguishes four dysfunctional patterns occurring in
the current research paradigm in the industry and academia alike.

\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
\item First, the inability to draw a clear distinction between speculation and explanation, with the first one often being disguised as the second. For instance, in a paper from 2017 [@steinhardt2017certified], Jacob Steinhardt, the author of @lipton2019troubling, admitted to stating that “the high dimensionality and abundance of irrelevant features... give the attacker more room to construct attacks” - although no experiments were conducted to measure what is the effect of dimensionality of the neural network on its attackability. 
\item Second, the inability of successful identification of the sources of performance improvement (whether it was problem formulation, optimization of the heuristics, data-preprocessing, hyperparameter tuning, or perhaps yet another aspect). As was shown by Gabor Melis, Chris Dyer, and Phil Blunsom, some improvements in language modeling which originally were ascribed to complex innovations in the architecture of the network, stem from hyperparameter tuning [@melis2017state]. As mentioned by @lipton2019troubling, there is a tremendous value coming from the thorough understanding of a particular method, and a variety of techniques are vital in the process (like ablation, robustness checks, qualitative error analysis) for the benefit of the whole community.
\item Third, "mathiness"---the use of obscure language and often covering weak argumentation with the alluring but often apparent depth of technical jargon. Again Jacob Steinhardt admitted infusing his 2015 paper co-authored with Percy Liang [@steinhardt2015learning] with an irrelevant theorem to amplify the empirical results. They discussed “staged strong Doeblin chains” which actually had limited pertinence to the learning algorithm---the main subject of a paper. 
\item Last but not least---misuse of language\todo{Give an example}. This includes suggestive definitions without proper explanation of what they mean in the context (e.g. inflating good performance in simple NLP tasks to human-level understanding), overloading the papers with technical terminology, or suitcase words (words that can encompass a variety of meanings, e.g. consciousness) 
 \end{enumerate}

Yet another obstacle in the process is the lack of gold standard in
dataset creation and taxonomies of abusive language being used for
instance in the process of annotating different datasets. Frequently
people obtain data from various sources and do not follow any
universally used instructions when it comes to annotation, leading to
discrepancies between various datasets being tagged within one domain
(e.g.~hate speech). Lack of expert annotators and proper annotation
criteria and instructions are also widespread, with the common practice
of hiring untrained workers from Mechanical Turk or other crowdsourcing
platforms.

Although there are some initiatives developed in response, most notably,
functional tests for Hate Speech Detection Models created by Röttger et
al. (2020), or the Online Safety Data Initiative (OSDI) (\emph{About the
Online Safety Data Initiative}, n.d.), focused on projects related to
improving access to data, standardizing the description of online harms,
as well as creating tools and benchmarks for evaluation of technologies
focused on safety, much effort must be made before wider adoption of
such solutions comes into force.

At the same time, only automated methods can scan through the massive
amount of content being generated every day on different platforms. On
Facebook, there are more than 3B comments and likes daily
({``Facebook,''} 2012), 500M tweets are sent daily on Twitter (\emph{10
{Twitter} {Statistics} {Every} {Marketer} {Should} {Know} in 2021
{[}{Infographic}{]}}, 2021), and over 2B comments made by users of
Reddit in 2020 ({``Reddit in 2020,''} 2020) which is almost 3M comments
made daily. With this amount of content, it's either impossible or
extremely costly to scale the moderation workforce. One can also have
doubts about the ethical aspects of hiring workers who are often unaware
of how this kind of task will affect their well-being. Being submersed
in the cyber-Augean stables takes a toll on many---as examined by
Roberts (2014) \& Roberts (2016). Workers hired for such tasks are often
low-status and low-wage, isolated and asked to keep what they've seen in
secret under restrictive NDAs. This in turn makes the research in the
area extremely difficult, since moderators are not allowed to talk about
their work conditions or any other related subject. Those who decided to
break the NDA are risking a penalty. Screening through the reported
user-generated content is connected with exposure to violent and deeply
disturbing materials, with child pornography, murders, or suicides as
examples of the most extreme cases. This can lead to serious
psychological damage, such as depression, or PTSD (Roberts, 2014).
Although there are certain initiatives being developed or introduced to
reduce the emotional impact of the moderation, like stylistic
alterations to a content (applying grayscale or blurring to images)
(Karunakaran \& Ramakrishan, 2019), workplace wellness programs,
clinical support, or psychological training (Steiger, Bharucha,
Venkatagiri, Riedl, \& Lease, 2021), none of the methods can eliminate
the psychological distress completely. Some of the employees filed a
lawsuit against Facebook and as a result, the company agreed to pay
\$52M in compensation for mental health issues developed during the job
(Newton, 2020). Also as described by Parks (2019), the work is often
performed under time pressure, reviewing 25K pieces of content per day.
Spending on average three to five seconds on each image reported for
moderation might not lead to the most thoughtful decisions and as shown
Stepanikova (2012), high time pressure can amplify human biases. Taking
into consideration that Facebook employs 15K moderators (Koetsier, 2020)
and most likely more are needed to keep up with the growing amount of
content, with the parallel considerations about the negative effects of
content moderation on mental health, a collaboration between humans and
machines in this area seems inevitable.

\hypertarget{pro-active-and-reactive-moderation}{%
\section{Pro-active and reactive
moderation}\label{pro-active-and-reactive-moderation}}

There are different approaches when it comes to the moderation of online
content. One can follow the workflow of reactive moderation, which
happens once the content is published. Harmful messages can be either
reported by the users of the platforms or automated methods and then
sent for review to moderators. A set of actions can be then taken
depending on the platform and their community guidelines---on the
content or user level. A harmful message can be deleted, made invisible
to other users, or certain profanities can be altered with special signs
to censor them. Depending on the type and amount of infraction, a
particular user can be warned, muted, shadowbanned, or banned from
further participation in the community for a period of time. The
weakness of the reactive method is that the damage is done. Whoever is
the recipient of the abusive message has the chance to see it and
potentially suffer Wachs et al. (2019). Yet another weakness connected
with relying solely on human reports is the content that is harmful but
unreported by a receipted or any bystander. Although the exact scale of
unreported content is not known, various self-report studies show that a
lot of children, teens, or even adults do not report cyberbullying or
harassment online ({``Free to {Play}?''} 2020; French, 2021; {``In,''}
2017).

Yet another type of moderation can be distinguished as pro-active or
pre-moderation. In pre-moderation, automated methods are either based on
Artificial Intelligence or other less sophisticated tools
(e.g.~blacklists) and can screen the content before it gets published.
If a type of harmful message gets detected, it can be removed before
reaching the recipient. Due to the aforementioned dubious performance of
state-of-the-art statistical methods, particularly low precision, they
are rarely used autonomously.

Pro-active moderation can be utilized using AI or other methods to
promote socially positive engagement. Instead of or in the combination
with punitive solutions such as privileges restriction, one can induce
empathy or community norms with counter-speech. Counter speech as
described by Dangerous Speech Project ({``Counterspeech {{}} {Dangerous}
{Speech} {Project},''} 2017) is ``any direct response to hateful or
harmful speech which seeks to undermine it.'' As examined by Munger
(2017), counter-speech can be effective in the reduction of racist
tweets (although only in the condition in which a white male with high
followers was approaching another white male). In a study conducted by
Bilewicz et al. (2021), a bot disguised as a Reddit user, equipped with
normative and empathetic interventions, significantly decreased the
amount of personal attacks generated on Reddit. In yet another study by
Miškolci, Kováčová, \& Rigová (2020), this technique was not effective
in changing the behavior of the users (counter-speech here aimed at
reducing the prejudice against Roma minority in Slovakia), but
encouraged bystanders to express pro-Roma comments on specific Facebook
posts. Counter-speech also has been shown to have the potential to
increase civility online in studies conducted by Friess, Ziegele, \&
Heinbach (2021), Molina \& Jennings (2018), Han, Brazeal, \& Pennington
(2018). \ldots{}

\hypertarget{collective-intelligence-approach-to-counter-speech}{%
\section{Collective Intelligence Approach to
Counter-speech}\label{collective-intelligence-approach-to-counter-speech}}

Traditionally collective intelligence has been defined as ``a group or a
team's combined capacity and capability to perform a wide variety of
tasks and solve diverse problems'' ({``Collective {Intelligence},''}
n.d.). In our paper and in the theoretical underpinnings of the
experiment itself, we will be relying on a collective intelligence scope
proposed by Nesta, an innovation foundation
(\url{https://www.nesta.org.uk}), which focuses on a collaboration
between human and machine intelligence to develop innovative solutions
to social challenges.

The main objective of the experiment was to test whether the level of
verbal aggression (personal attacks) of a group of users' regularly
attacking others on Reddit can be significantly decreased by
community-driven, counter-speech interventions conducted by volunteers
in partnership with Artificial Intelligence. Instead of using negative
motivation system, the assumption was to test a positive one -
convincing verbally violent users to refrain from using cyberviolence
based on peer-pressure regulation and experiential learning of a
positive set of norms and empathy. Algorithms developed for the
detection of personal attacks were used to monitor the activity of
experimental groups and notify volunteers about all attacks generated by
its' members. Volunteers, after receiving a notification on Slack, could
then react with a proper intervention. Such an approach served as a
distributed bottom-up voluntary model of moderation based on collective
intelligence---utilizing human + machine intelligence.

In the end, what we were able to compare was the following: the
effectiveness of the existing Reddit moderation system (predominantly
grounded in a punitive authoritarian paradigm) versus the existing
moderation system combined with collective intelligence---Artificial
Intelligence supported with a crowd of volunteers---who introduced the
element of positive peer-pressure.

\textbf{Empathetic interventions} In the first treatment condition,
volunteers were asked to send empathy-inducing messages focusing either
on the target of verbal aggression (e.g.~„Hey such words might hurt the
other person''), stressing the common humanity aspect that we call share
(„We are all humans of flesh and blood''), or even infusing the
intervention with the emphatic response to the attacker (Hey I
understand your strong emotions\ldots''). At the core of the empathetic
interventions, we put forth the notion that goes back to David Hume and
Adam Smith. The first one conceived empathy (at the time referred to as
sympathy) as mirroring the emotional state of another person. In the
academic psychological literature, similar phenomenon was distinguished
and coined in the term emotional contagion
(\url{https://www.sciencedirect.com/topics/psychology/emotional-contagion}):
``the process in which an observed behavioral change in one individual
leads to the reflexive production of the same behavior by other
individuals in close proximity, with the likely outcome of converging
emotionally.''

For Adam Smith, sympathy consisted of visualizing how the sympathetic
person would feel in the particular circumstances of the other --- thus
here the process was based not so much on mirroring, but rather
projecting my imagination of what it is like to be that person in a
certain moment). Without further delving into the differentiation
between those two, sympathy in both accounts is crucial in the
constitution of human beings as social and moral creatures
(\url{https://plato.stanford.edu/entries/empathy/}). It enables the
emotional connection to others and concern for their well-being. The
interventions were supposed to serve the role of empathetic response
enablers --- reminding the person in front of the screen that there is a
real human being on the other side.

In the psychological literature, various kinds of empathetic responses
were distinguished - the aforementioned emotional contagion,
affective/proper empathy, sympathy, personal distress or cognitive
empathy ((\url{https://plato.stanford.edu/entries/empathy/}). During the
experiment, we could not observe or measure whether the empathetic
response indeed was evoked. The study was conducted in a digital setting
--- interventions were most likely read in the private space, in front
of personal computers or smartphones. Even if we would change the
setting to a lab, differentiating and measuring empathy is not trivial
if not impossible (to check).

Also, interventions that stated that ``such words might hurt the other
person'' have the underlying assumption that the receiver of the attack
might be hurt, but in reality that might not even be the case.
Intuitively, and following the argumentation of Thomas Nagel in ``What
is it like to be a bat?'' one can only imagine what it is like --- but
for me --- to be a bat or to be a receiver of the attack. To each
observed or imagined experience there is an array of subjective quality
to actually experiencing it and in this way receiving a particular
message might be met with a unique reception by each conscious being.
Also, full epistemic access to the mind and body of another is
impossible.

But even though such access is impossible and putting aside the broad
spectrum of phenomena related to empathetic response, our goal was by
utilizing interventions referring to empathy in various forms, changing
the behavior of the attacker --- convincing him to refrain from using
verbal aggression towards others. Whether the message indeed gave rise
to empathy was not of importance --- one can imagine a hypothetical
scenario of a successful intervention in which the attacker changed his
behavior long-term and stopped attacking others, hopefully as a result
of interventions, but the empathetic response was not even evoked. The
behavioral change could be induced by other motivations ---
e.g.~unwillingness from the exclusion from the community. Thus the goal
is not to evoke empathy, but through empathetic interventions ---
substantially limit the use of verbal aggression among study group
members.

\textbf{Normative interventions} In the second treatment condition,
volunteers were asked to send norms-inducing messages written on the
grounds of deontology (e.g.~„we have a duty to respect each other during
the discussions``), virtue ethics (e.g.~„capacity to be respectful in a
heated discussion is a virtue and requires hard work'') or Reddit's
community guideline (e.g.~``Let's recall the reddiquette and adhere to
the same standards of behavior online that you follow in real life'').

Whatever the strategy was used, it was supposed to express the
unacceptability of verbal aggression in a direct or non-direct way. A
more direct approach referred to deontology in which „actions are good
or bad according to a clear set of rules''
(\url{https://ethics.org.au/ethics-explainer-deontology/}). The basic
assumption behind those interventions is that there is something we
ought to do or are morally required to do --- and such actions are the
right actions. There are also actions we ought not to do --- and such
actions are morally wrongful. This line of thinking can be traced back
to Immanuel Kant who thought that all universal moral obligations can
stem from categorical imperative: ``act only in accordance with that
maxim through which you can at the same time will that it become a
universal law'' (Groundwork of the Metaphysic of Morals).

In the psychological literature, Robert B. Cialdini distinguished three
types of norms that he found to be effective in guiding human action:
descriptive norms (focused on the perception of how the majority of
people would behave), injunctive (based on the perception of how the
majority of people would approve or disapprove certain conduct), or
personal norms (based on the perception of how a particular person would
approve or disapprove certain behavior) CITE:
\url{https://www.sciencedirect.com/science/article/abs/pii/S0065260108603305}.
Although interventions written on the grounds of duties and deontology
cannot be reduced to any of the above, since following Hume's guillotine
--- from the fact that the majority of people are respectful does not
follow the normative statement of civility --- I hoped that expressing a
universal normative statement will be even more powerful and potentially
spawn an interpretation of social acceptability or unacceptability of
certain behavior.

Yet other interventions in normative group were encouraged to be
expressed in the light of virtue ethics and stressed the importance of
adhering to or practicing certain virtues. Such a message was assumed to
motivate the moral agent in the process of positive reinforcement and
create a feeling of the desirability of certain behaviors, like kindness
and civility. The core assumption here is that the virtue itself is not
genetically inherited but is rather a potential or a disposition of a
character that can be practice and mastered like a practical skill (even
though as mentioned by Natasza Szutta in {[}cite her book about virtue
ethics{]} virtues and practical skills share some fundamental
differences). NS distinguished two types of virtues - affective and
cognitive. Affective virtues encompass the emotions and feelings that
play an important and positive role in morality and can act as a support
in the course of becoming a virtuous man. A cognitive virtue relates to
the intellectual aspect in which one knows how to act in certain
situations and understands the rules of morally rightful actions
(e.g.~what is kindness and how a kind man acts).

Here, just as in the case of empathetic interventions, any attempts to
measure whether the target of the intervention in the case of a positive
outcome (behavioral change) indeed acted in a virtuous way. A virtue of
kindness may manifest itself in particular behaviors but as such cannot
be identified with the virtuous deed, as highlighted by Natasza Szutta.
Although again, the goal of the experiment itself was to change the
behavior of the attackers, and measuring whether particular messages
contributed to more flourishing individuals in terms of virtue
development and character creation lies beyond the scope of this work.
Interventions could affect people either way - stimulate their moral
reflections or limit their attacks due to consequentialist way of
thinking and fear of potential outcomes (e.g.~being excluded from the
community).

Additionally, Reddit created their social etiquette called
``Rediquette''
(\url{https://www.reddithelp.com/hc/en-us/articles/205926439}) which is
``an informal expression of the values of many redditors, as written by
redditors themselves.'' All users are encouraged to abide by it and
moderators of communities (called subreddits) existing as a part of the
platform have the authority to exclude its members based either on the
basis of breaking the rules of the reddiquette or any other local rules
imposed by specific subreddit. Volunteers were encouraged to refer to
those norms as well, citing specific points, for instance, ``Hey there,
have you read the reddiquette? It says remember the human.''

As we have seen, those two categories of interventions - normative and
empathetic - encompass a whole variety of categories within. One might
think that it would be useful to construct a more diverse set of
treatment conditions in which we test each of the ethics or empathy
differentiation separately - e.g.~one for virtue ethics, deontology,
perhaps utilitarian approach, empathy toward the receiver of the attack,
empathy towards the attacker, so on and so forth. Additionally,
different types of interventions could be tested and tailored depending
on the spectrum on which one can be found in the foundations of moral
reasoning, as proposed by Jonathan Haidt: Care/Harm, Fairness/Cheating,
Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and
Liberty/Oppression (J. Haidt, The Righteous Mind). Albeit for the sake
of the sanity of our volunteers, we decided to create only two but broad
categories and let volunteers decide based on the context of the
conversation, how they want to respond to the attacker.

Cyberviolence was defined in this experiment as a personal attack - any
kind of verbal harassment, insult, or threat directed against the
interlocutor in a text-based conversation online. Those were detected
using Samurai Labs' cyberviolence detection system.

The following hypotheses were formulated:

H1: If a group of human volunteers notified by an AI-based cyberviolence
detection system about cyberviolence generated by the treatment group
users (cyberviolence will be defined as a personal attack, harassment,
or a threat targeted against an interlocutor) responds with
counter-speech interventions, this will result in a decreased
cyberviolence level for the whole group after the intervention period.

H2: If two groups receive different types of interventions
(empathy-based or normative), then the decrease in cyberviolence will be
larger in the case of normative interventions in comparison to the
empathy-based ones.

\hypertarget{experimental-design-and-data-collection}{%
\section{Experimental Design and data
collection}\label{experimental-design-and-data-collection}}

This was a 6-months field experiment in a digital setting conducted on a
popular Q\&A and news forum, Reddit (www.reddit.com). We formed
treatment and control groups based on three main criteria:

\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
 \item During the intervention period, we have expected to have 20 active volunteers at any given time, each willing to conduct 10 interventions daily. Thus, we needed approximately 200 attacks daily generated by the treatment groups.
\item The identification of users who regularly attack others was necessary to measure the effect of interventions at the end.
\item The identification of users who were active during the whole preliminary monitoring period was necessary to minimize the risk of attrition during the study. 
 \end{enumerate}

User identification process:

\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
   \item First, we obtained 1 week of real-time (coming from 15-22nd of February 2020), unmoderated data from Reddit. The content was downloaded from the data stream provided by pushshift.io. 
   \item Samurai Labs Artificial Intelligence for personal attacks detection was applied to identify users who attacked others at least once within the aforementioned timeframe. This resulted in the identification of 93966 users. 
   \item We removed all accounts which we suspected not to be run by humans (AutoModerator and all users which had "bot" in the username string). This resulted in the removal of 388 users, thus 93578 were left on our list.
   \item Next, we removed users who generated only 1 personal attack during the week (leaving only those who attacked at least twice). As mentioned, the group of those regularly attacking others was crucial to measure the effects of the interventions. This step resulted in the removal of users below the third quartile (Q3). 20124 users were left in our group.
   \item Moving forward, we removed users who generated less than 14 comments in this week. We cared about most active users, and 2 comments per day per person on average seemed reasonable (not sure yet how to justify this - 14 comments is below 1st quartile (Q1:28, Q2: 63, Q3:126, mean=103)). This resulted in the removal of 2192 users, so 17932 were left.
   \item We discarded users whose personal attacks to all comments ratio was below 2\%. This means the inclusion in the sample of users above the 1st quartile. 4422 users were removed, leaving us with a group of 13510. 
   \item The next step of the process begun on March 9th, 2020, and lasted until May 5th, 2020 (9 weeks). During this period we have monitored the activity of the identified group of 13510 users and applied further selection criteria to make sure we select those who were regularly active and attacked other users. 
   \item The period of monitoring was divided into weeks. We have discarded those weeks during which technical difficulties occurred with the pushshift.io (resulting in missing data). Thus, we have taken into consideration only 6 full weeks for the period.
   \item Users who generated at least 1 attack during 5 out of 6 weeks were identified. First, we planned to restrict the list to only those users, who generate at least 1 attack during each week (6/6) but such restrictive criterion led to only 255 users left, which was not enough for the study. The less restrictive criterion (at least 1 attack generated during 5/6 weeks) resulted in 694 people. 
   \item Next, we calculated the daily average number of personal attacks generated by the group who met the above criteria (which resulted in 357 attacks per day, 1.94 attacks daily per person on average).
   \item Knowing that we need around 200 attacks/daily per treatment group (just enough for volunteers to keep up according to our assumption), we have randomly selected 195 users per each treatment group (normative and empathetic). The rest was delegated as a control group (304 users). 
   
 \end{enumerate}

The duration of the experiment, 6 months, was divided into three
2-months periods. The first two months served as a monitoring period to
properly select groups and establish baselines. The next 2 months served
as treatment period, during which groups received counter-speech
comments from volunteers, in response to personal attacks detected by
the Artificial Intelligence-based system. The last 2-months served as
the post-treatment monitoring period to gather the data needed to
evaluate the effectiveness of interventions.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\vspace{-3mm}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-noauthor_10_2021}{}%
\emph{10 {Twitter} {Statistics} {Every} {Marketer} {Should} {Know} in
2021 {[}{Infographic}{]}}. (2021). Retrieved from
\url{https://www.oberlo.com/blog/twitter-statistics}

\leavevmode\hypertarget{ref-onlinesafetydata}{}%
\emph{About the online safety data initiative}. (n.d.).

\leavevmode\hypertarget{ref-barlett2021comparing}{}%
Barlett, C. P., Simmers, M. M., Roth, B., \& Gentile, D. (2021).
Comparing cyberbullying prevalence and process before and during the
COVID-19 pandemic. \emph{The Journal of Social Psychology}, 1--11.

\leavevmode\hypertarget{ref-bhattacharya}{}%
Bhattacharya, A. (2021). How {Covid}-19 lockdowns weakened {Facebook}'s
content moderation algorithms. Retrieved from
\url{https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}

\leavevmode\hypertarget{ref-bilewicz2021artificial}{}%
Bilewicz, M., Tempska, P., Leliwa, G., Dowgiałło, M., Tańska, M.,
Urbaniak, R., \& Wroczyński, M. (2021). Artificial intelligence against
hate: Intervention reducing verbal aggression in the social network
environment. \emph{Aggressive Behavior}, \emph{47}(3), 260--266.

\leavevmode\hypertarget{ref-binns2017like}{}%
Binns, R., Veale, M., Van Kleek, M., \& Shadbolt, N. (2017). Like
trainer, like bot? Inheritance of bias in algorithmic content
moderation. \emph{International Conference on Social Informatics},
405--415. Springer.

\leavevmode\hypertarget{ref-noauthor_collective_nodate}{}%
Collective {Intelligence}. (n.d.). Retrieved from
\url{https://www.oxford-review.com/oxford-review-encyclopaedia-terms/collective-intelligence/}

\leavevmode\hypertarget{ref-noauthor_counterspeech_2017}{}%
Counterspeech {{}} {Dangerous} {Speech} {Project}. (2017). Retrieved
from \url{https://dangerousspeech.org/counterspeech/}

\leavevmode\hypertarget{ref-noauthor_facebook:_2012}{}%
Facebook: 3.2 {Billion} {Likes} \& {Comments} {Every} {Day}. (2012).
Retrieved from
\url{https://martech.org/facebook-3-2-billion-likes-comments-every-day/}

\leavevmode\hypertarget{ref-noauthor_free_nodate}{}%
Free to {Play}? {Hate}, {Harassment} and {Positive} {Social}
{Experience} in {Online} {Games} 2020. (2020). Retrieved from
\url{https://www.adl.org/free-to-play-2020}

\leavevmode\hypertarget{ref-french_as_2021}{}%
French, C. (2021). As the pandemic forces us online, {LGBTQ2S}+ teens
deal with cyberbullying. Retrieved from
\url{https://www.ctvnews.ca/canada/as-the-pandemic-forces-us-online-lgbtq2s-teens-deal-with-cyberbullying-1.5430945}

\leavevmode\hypertarget{ref-friess2021collective}{}%
Friess, D., Ziegele, M., \& Heinbach, D. (2021). Collective civic
moderation for deliberation? Exploring the links between citizens'
organized engagement in comment sections and the deliberative quality of
online discussions. \emph{Political Communication}, 1--23.

\leavevmode\hypertarget{ref-gerrard2020covid19}{}%
Gerrard, Y. (2020). \textless? covid19?\textgreater{} The COVID-19
mental health content moderation conundrum. \emph{Social Media+
Society}, \emph{6}(3), 2056305120948186.

\leavevmode\hypertarget{ref-geva2019we}{}%
Geva, M., Goldberg, Y., \& Berant, J. (2019). Are we modeling the task
or the annotator? An investigation of annotator bias in natural language
understanding datasets. \emph{arXiv Preprint arXiv:1908.07898}.

\leavevmode\hypertarget{ref-grant_2021}{}%
Grant, J. (2021). Australia's eSafety commissioner targets abuse online
as covid-19 supercharges cyberbullying \textbar{} the strategist.
Retrieved from
\url{https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}

\leavevmode\hypertarget{ref-grondahl2018all}{}%
Gröndahl, T., Pajola, L., Juuti, M., Conti, M., \& Asokan, N. (2018).
All you need is" love" evading hate speech detection. \emph{Proceedings
of the 11th ACM Workshop on Artificial Intelligence and Security},
2--12.

\leavevmode\hypertarget{ref-han2018civility}{}%
Han, S.-H., Brazeal, L. M., \& Pennington, N. (2018). Is civility
contagious? Examining the impact of modeling in online political
discussions. \emph{Social Media+ Society}, \emph{4}(3),
2056305118793404.

\leavevmode\hypertarget{ref-hoff2009cyberbullying}{}%
Hoff, D. L., \& Mitchell, S. N. (2009). Cyberbullying: Causes, effects,
and remedies. \emph{Journal of Educational Administration}.

\leavevmode\hypertarget{ref-noauthor_:game_nodate}{}%
In:{Game} {Abuse}. (2017). Retrieved from
\url{https://www.ditchthelabel.org/research-papers/ingame-abuse/}

\leavevmode\hypertarget{ref-jordan2015machine}{}%
Jordan, M. I., \& Mitchell, T. M. (2015). Machine learning: Trends,
perspectives, and prospects. \emph{Science}, \emph{349}(6245), 255--260.

\leavevmode\hypertarget{ref-karunakaran2019testing}{}%
Karunakaran, S., \& Ramakrishan, R. (2019). Testing stylistic
interventions to reduce emotional impact of content moderation workers.
\emph{Proceedings of the AAAI Conference on Human Computation and
Crowdsourcing}, \emph{7}, 50--58.

\leavevmode\hypertarget{ref-keipi2016online}{}%
Keipi, T., Näsi, M., Oksanen, A., \& Räsänen, P. (2016). \emph{Online
hate and harmful content: Cross-national perspectives}. Taylor \&
Francis.

\leavevmode\hypertarget{ref-koetsier_report}{}%
Koetsier, J. (2020). Report: {Facebook} {Makes} 300,000 {Content}
{Moderation} {Mistakes} {Every} {Day}. Retrieved from
\url{https://www.forbes.com/sites/johnkoetsier/2020/06/09/300000-facebook-content-moderation-mistakes-daily-report-says/}

\leavevmode\hypertarget{ref-noauthor_l1ght_2020}{}%
L1ght. (2020). L1ght releases groundbreaking report on corona-related
hate speech and online toxicity. Retrieved from
\url{https://l1ght.com/l1ght-releases-groundbreaking-report-on-corona-related-hate-speech-and-online-toxicity/}

\leavevmode\hypertarget{ref-Zachary_hate}{}%
Laub, Z. (2019). Hate {Speech} on {Social} {Media}: {Global}
{Comparisons}. Retrieved from
\url{https://www.cfr.org/backgrounder/hate-speech-social-media-global-comparisons}

\leavevmode\hypertarget{ref-lecun2015deep}{}%
LeCun, Y., Bengio, Y., \& Hinton, G. (2015). Deep learning.
\emph{Nature}, \emph{521}(7553), 436--444.

\leavevmode\hypertarget{ref-lipton2019troubling}{}%
Lipton, Z. C., \& Steinhardt, J. (2019). Troubling trends in machine
learning scholarship: Some ML papers suffer from flaws that could
mislead the public and stymie future research. \emph{Queue},
\emph{17}(1), 45--77.

\leavevmode\hypertarget{ref-macavaney2019hate}{}%
MacAvaney, S., Yao, H.-R., Yang, E., Russell, K., Goharian, N., \&
Frieder, O. (2019). Hate speech detection: Challenges and solutions.
\emph{PloS One}, \emph{14}(8), e0221152.

\leavevmode\hypertarget{ref-mehrabi2021survey}{}%
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., \& Galstyan, A.
(2021). A survey on bias and fairness in machine learning. \emph{ACM
Computing Surveys (CSUR)}, \emph{54}(6), 1--35.

\leavevmode\hypertarget{ref-mivskolci2020countering}{}%
Miškolci, J., Kováčová, L., \& Rigová, E. (2020). Countering hate speech
on facebook: The case of the roma minority in slovakia. \emph{Social
Science Computer Review}, \emph{38}(2), 128--146.

\leavevmode\hypertarget{ref-molina2018role}{}%
Molina, R. G., \& Jennings, F. J. (2018). The role of civility and
metacommunication in facebook discussions. \emph{Communication Studies},
\emph{69}(1), 42--66.

\leavevmode\hypertarget{ref-munger2017tweetment}{}%
Munger, K. (2017). Tweetment effects on the tweeted: Experimentally
reducing racist harassment. \emph{Political Behavior}, \emph{39}(3),
629--649.

\leavevmode\hypertarget{ref-musgrave2020metric}{}%
Musgrave, K., Belongie, S., \& Lim, S.-N. (2020). A metric learning
reality check. \emph{European Conference on Computer Vision}, 681--699.
Springer.

\leavevmode\hypertarget{ref-newton_facebook_2020}{}%
Newton, C. (2020). Facebook will pay \$52 million in settlement with
moderators who developed {PTSD} on the job. Retrieved from
\url{https://www.theverge.com/2020/5/12/21255870/facebook-content-moderator-settlement-scola-ptsd-mental-health}

\leavevmode\hypertarget{ref-parks2019dirty}{}%
Parks, L. (2019). Dirty data: Content moderation, regulatory
outsourcing, and the cleaners. \emph{Film Quarterly}, \emph{73}(1),
11--18.

\leavevmode\hypertarget{ref-noauthor_reddit_nodate}{}%
Reddit in 2020. (2020). Retrieved from
\url{https://www.reddit.com/r/blog/comments/k967mm/reddit_in_2020/}

\leavevmode\hypertarget{ref-roberts2014behind}{}%
Roberts, S. T. (2014). \emph{Behind the screen: The hidden digital labor
of commercial content moderation}. University of Illinois at
Urbana-Champaign.

\leavevmode\hypertarget{ref-roberts2016commercial}{}%
Roberts, S. T. (2016). \emph{Commercial content moderation: Digital
laborers' dirty work}.

\leavevmode\hypertarget{ref-rosa2019automatic}{}%
Rosa, H., Pereira, N., Ribeiro, R., Ferreira, P. C., Carvalho, J. P.,
Oliveira, S., \ldots{} Trancoso, I. (2019). Automatic cyberbullying
detection: A systematic review. \emph{Computers in Human Behavior},
\emph{93}, 333--345.

\leavevmode\hypertarget{ref-rottger2020hatecheck}{}%
Röttger, P., Vidgen, B., Nguyen, D., Waseem, Z., Margetts, H., \&
Pierrehumbert, J. (2020). Hatecheck: Functional tests for hate speech
detection models. \emph{arXiv Preprint arXiv:2012.15606}.

\leavevmode\hypertarget{ref-schmidt2017survey}{}%
Schmidt, A., \& Wiegand, M. (2017). A survey on hate speech detection
using natural language processing. \emph{Proceedings of the Fifth
International Workshop on Natural Language Processing for Social Media},
1--10.

\leavevmode\hypertarget{ref-sejnowski2020unreasonable}{}%
Sejnowski, T. J. (2020). The unreasonable effectiveness of deep learning
in artificial intelligence. \emph{Proceedings of the National Academy of
Sciences}, \emph{117}(48), 30033--30038.

\leavevmode\hypertarget{ref-sorrentino2019epidemiology}{}%
Sorrentino, A., Baldry, A. C., Farrington, D. P., \& Blaya, C. (2019).
Epidemiology of cyberbullying across europe: Differences between
countries and genders. \emph{Educational Sciences: Theory \& Practice},
\emph{19}(2).

\leavevmode\hypertarget{ref-steiger2021psychological}{}%
Steiger, M., Bharucha, T. J., Venkatagiri, S., Riedl, M. J., \& Lease,
M. (2021). The psychological well-being of content moderators.
\emph{Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems, CHI}, \emph{21}.

\leavevmode\hypertarget{ref-stepanikova2012racial}{}%
Stepanikova, I. (2012). Racial-ethnic biases, time pressure, and medical
decisions. \emph{Journal of Health and Social Behavior}, \emph{53}(3),
329--343.

\leavevmode\hypertarget{ref-swamy2019studying}{}%
Swamy, S. D., Jamatia, A., \& Gambäck, B. (2019). Studying
generalisability across abusive language detection datasets.
\emph{Proceedings of the 23rd Conference on Computational Natural
Language Learning (CoNLL)}, 940--950.

\leavevmode\hypertarget{ref-vogels_state_2021}{}%
Vogels, E. A. (2021). The {State} of {Online} {Harassment}. Retrieved
from
\url{https://www.pewresearch.org/internet/2021/01/13/the-state-of-online-harassment/}

\leavevmode\hypertarget{ref-wachs2019associations}{}%
Wachs, S., Wright, M. F., Sittichai, R., Singh, R., Biswal, R., Kim, E.,
\ldots{} others. (2019). Associations between witnessing and
perpetrating online hate in eight countries: The buffering effects of
problem-focused coping. \emph{International Journal of Environmental
Research and Public Health}, \emph{16}(20), 3992.

\leavevmode\hypertarget{ref-wu2019errudite}{}%
Wu, T., Ribeiro, M. T., Heer, J., \& Weld, D. S. (2019). Errudite:
Scalable, reproducible, and testable error analysis. \emph{Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics}, 747--763.

\leavevmode\hypertarget{ref-yin2021towards}{}%
Yin, W., \& Zubiaga, A. (2021). Towards generalisable hate speech
detection: A review on obstacles and solutions. \emph{PeerJ Computer
Science}, \emph{7}, e598.

\end{CSLReferences}

\end{document}
