---
title: "Investigating the impact of interventionist causal approach on the study of verbal aggression online"
author: "Patrycja Tempska and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: yes
    df_print: kable
    keep_tex: yes
    includes:
      in_header: Rafal_latex6.sty
  html_document:
    df_print: paged
  word_document: default
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: ../references/attacks.bib
csl: ../references/apa-6th-edition.csl
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = '../')

#libraries used
library(ggplot2)
library(ggthemes)
library(gridExtra)

```



\tableofcontents


\vspace{1mm}
\footnotesize

\normalsize


# Introduction

We live in a world in which there is an abundance of data. Referring the phrase coined by Clive Humby, "data is the new oil" --- data has displaced oil and become the world’s most valuable resource. Albeit there are some fundamental differences between the two assets. Oil belongs to a very limited and finite resource, while data is growing at an exponential rate (cite: https://cloudtweaks.com/2015/03/how-much-data-is-produced-every-day/). The value of oil does not depend on its amount or the refinery processes - it's rather constant at a particular dynamic of market's supply and demand. Data and its value, on the contrary, is inherently tied to its amount and the "refinery process" - various tools and techniques for its discovery and analysis. In some cases, certain techniques can be used only if one has a sufficient (meaning enormous) data at one's disposal --- e.g. statistical methods like machine learning and deep learning \todo{@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}}. 
In the last couple of years, more and more industry and academia representatives alike started to research and develop tools for causal discovery and inference. Unfortunately, although thinking in causal terms seems to be our inherent human capacity and one of our basic cognitive abilities, that is not necessarily the case with the majority of toolkits and statistical models developed to draw insight and correlations from data. 
In classical statistics, which is stripped of causality-related terms and reasoning, one cannot speak about causal effects, unless one deals with a randomized controlled experiment. This comprises one of the limits of the classical paradigm in statistics (among others such as arbitrary thresholds of statistical significance, p-hacking, disregard of prior information, and others which we will not discuss here).
But often scientists conduct the so-called cross-sectional research, in which no variable is manipulated. In the language and toolkit of classical statistics in such cases we cannot conduct any causal inference. What we can measure using those methods is correlation, and, as frequently repeated, “correlation does not imply causation”. Correlation can be positive, negative, or there can be no correlation at all. A positive one implies that the increase of the value of one variable will influence the increase of another variables’ value. A negative correlation means that the change in values of two variables will diverge into separate directions --- one will increase and one will decrease alongside the first one. We can measure the strength of the correlation using various mathematical methods, where 1 or -1 means we are dealing with perfect correlation --- the value of one variable changes proportional to the value of another variable. But even though causation is necessarily connected with correlation, as already mentioned --- not every correlation implies causation. This is especially problematic if we take into consideration the fact that statisticians and people in general naturally think in causal terms and spurious interpretations of studies occur frequently. On the other hand, a conservative approach of many classical statisticians led to a long-living debate on whether smoking causes cancer, lasting for over 20 years. One of the reasons for such a fierce debate was the lack of a causal framework, which was still in its infancy at that time. Framework of tools with which we could validate or invalidate assumptions about the existence of the potential confounder --- a variable causing both craving cigarettes and lung cancer (e.g. some genetic factor), creating a spurious correlation between cigarettes and lung cancer.
Modeling causality and moving beyond traditional statistics is a relatively young endeavor. One of the pioneers of the work on causality is Judea Pearl. As he describes, causality and traditional statistics are governed by opposing ways of thinking. While statistics might be described as “study of methods of the reduction of data”, causality requires the incorporation of an extra layer --- the understanding or prior knowledge (assumptions) about the process that generate the data in the first place. Classical statistics might be described here as a “model-free approach,” just as Artificial Intelligence methods based on machine learning/deep learning, in which no prior knowledge can be taken into account. 

In this paper, I will show how mathematical methods for process modeling which includes assumptions about causal connections can be applied to discover how different variables affect each other, moving beyond classical statistics. Looking at the analysis from two perspectives --- traditional (classical, frequentist statistics) and bayesian -- we will see the advantage of the second paradigm in learning about the world based on the data we have at our disposal. Data used for analysis concerned in this paper comes from an experimental intervention study conducted in a naturalistic, digital setting (Q&A forum on Reddit), utilizing a collective intelligence approach to content moderation and reduction of the level of verbal aggression among a selected group of Reddit users who regularly attack other community members. Collective Intelligence in this sense means exploring the collaboration between human and machine intelligence to develop solutions to social challenges. Artificial Intelligence was used to detect verbal aggression (personal attacks) and notify human volunteers about attacks. Volunteers after receiving notifications employed interventions inspired in its underlying principles in philosophy and psychology. There were two broad categories of interventions - one based on norms (deontology, virtue ethics among others) and the second based on empathy (inspired by notions of David Hume and Adam Smith). 

One might also wonder about the motivations for conducting such an experiment. Much effort has been made in order to tackle the problem of verbal aggression and harassment online, but looking at various reports and surveys [@sorrentino2019epidemiology; @vogels_state_2021; @Zachary_hate], it remains a common hindrance for people engaging with social media in their everyday lives. The problem got exacerbated in the midst of the COVID19 pandemic, during which the majority of our social life moved to cyberspace. During this shift, there was an increase in cyberbullying attitudes and perpetration [@barlett2021comparing], 90% increase in public reports of illegal online content [@grant_2021], including 114% increase in non-consensual sharing of intimate images, 30% increase in cyberbullying, as well as 40% of increase in adults reporting online harassment. According to a report conducted by company L1ght [@noauthor_l1ght_2020], hate speech directed towards China and the Chinese went up by 900% on Twitter. Gaming platforms were in the spotlight as well, with a 40% increase in toxicity on Discord \footnote{In both reports, the increase is reported as a relative change between the year 2019 and 2020, with no absolute indicators.}.

But alongside the growing need for even more efficient and proactive moderation, the capacity to execute it did not go hand in hand, forcing companies and policymakers to rethink the current model of moderation processes and workforce. Due to the COVID19 restrictions including social distancing, a lot of those serving the role of moderators had to be sent home [@bhattacharya] without the ability to work remotely because of the constraints affiliated with restrictive non-disclosure agreements (NDA) among others. Curtailing the moderators' workforce was accompanied by more agency given to algorithms and AI-based moderation. Those changes, as argued by @gerrard2020covid19, can be seen as a serious red flag in terms of safety for all users on online platforms. That is why in the paper I will also try to sketch a landscape of current moderation techniques through a critical lens.

# Automated versus Human-based Moderation
The hindrances and threats that go along with the Artificial Intelligence-based methods for moderation have been widely debated, with the most critical discussions revolving around technology performance [@macavaney2019hate; @schmidt2017survey]. State-of-the-art solutions are mostly governed by statistical methods including deep learning and machine learning [@lecun2015deep; @sejnowski2020unreasonable; @jordan2015machine]. Their performance is inherently tied to the amount of data being fed to the system and the quality of its annotation. 
\todo{This deserves a developed paragraph with examples}At different stages of the process, from data gathering and preparation, annotation to the training or algorithms themselves, biases seem to be omnipresent [@binns2017like, @geva2019we @mehrabi2021survey].

\todo{This deserves a developed paragraph with examples}Users' of online services are also creative in their strategies to circumvent automated content moderation systems and as shown by @grondahl2018all, current techniques are vulnerable to the most common evasion attacks like word changes (inseting typos and leetspeak), word-boundary changes (inserting or removing whitespace), or word appending (appending common or non-hateful words like "love"). 

\todo{This deserves a developed paragraph with examples}Lack of generalisability of the models---the ability to perform well on datasets coming from sources other than the one used for training---is a serious shortcoming as well [@yin2021towards; @swamy2019studying; @rosa2019automatic]. 

As shown by @wu2019errudite, @lipton2019troubling, and @musgrave2020metric in practice, the development of such models often lacks thorough error analysis and legitimate experimental methodology, which can result in non-reproducibility. This is also connected with a potential lack of thorough understanding of the limitations of the models and spurious conclusions being announced to a wider public. Specifically, @lipton2019troubling distinguishes four dysfunctional patterns occurring in the current research paradigm in the industry and academia alike. 

\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
\item First, the inability to draw a clear distinction between speculation and explanation, with the first one often being disguised as the second. For instance, in a paper from 2017 [@steinhardt2017certified], Jacob Steinhardt, the author of @lipton2019troubling, admitted to stating that “the high dimensionality and abundance of irrelevant features... give the attacker more room to construct attacks” - although no experiments were conducted to measure what is the effect of dimensionality of the neural network on its attackability. 
\item Second, the inability of successful identification of the sources of performance improvement (whether it was problem formulation, optimization of the heuristics, data-preprocessing, hyperparameter tuning, or perhaps yet another aspect). As was shown by Gabor Melis, Chris Dyer, and Phil Blunsom, some improvements in language modeling which originally were ascribed to complex innovations in the architecture of the network, stem from hyperparameter tuning [@melis2017state]. As mentioned by @lipton2019troubling, there is a tremendous value coming from the thorough understanding of a particular method, and a variety of techniques are vital in the process (like ablation, robustness checks, qualitative error analysis) for the benefit of the whole community.
\item Third, "mathiness"---the use of obscure language and often covering weak argumentation with the alluring but often apparent depth of technical jargon. Again Jacob Steinhardt admitted infusing his 2015 paper co-authored with Percy Liang [@steinhardt2015learning] with an irrelevant theorem to amplify the empirical results. They discussed “staged strong Doeblin chains” which actually had limited pertinence to the learning algorithm---the main subject of a paper. 
\item Last but not least---misuse of language\todo{Give an example}. This includes suggestive definitions without proper explanation of what they mean in the context (e.g. inflating good performance in simple NLP tasks to human-level understanding), overloading the papers with technical terminology, or suitcase words (words that can encompass a variety of meanings, e.g. consciousness) 
 \end{enumerate}

Yet another obstacle in the process is the lack of gold standard in dataset creation and taxonomies of abusive language being used for instance in the process of annotating different datasets. Frequently people obtain data from various sources and do not follow any universally used instructions when it comes to annotation, leading to discrepancies between various datasets being tagged within one domain (e.g. hate speech). Lack of expert annotators and proper annotation criteria and instructions are also widespread, with the common practice of hiring untrained workers from Mechanical Turk or other crowdsourcing platforms.

Although there are some initiatives developed in response, most notably, functional tests for Hate Speech Detection Models created by @rottger2020hatecheck, or the Online Safety Data Initiative (OSDI) [@onlinesafetydata], focused on projects related to improving access to data, standardizing the description of online harms, as well as creating tools and benchmarks for evaluation of technologies focused on safety, much effort must be made before wider adoption of such solutions comes into force.  

At the same time, only automated methods can scan through the massive amount of content being generated every day on different platforms. On Facebook, there are more than 3B comments and likes daily [@noauthor_facebook:_2012], 500M tweets are sent daily on Twitter [@noauthor_10_2021], and over 2B comments made by users of Reddit in 2020 [@noauthor_reddit_nodate] which is almost 3M comments made daily. With this amount of content, it's either impossible or extremely costly to scale the moderation workforce. One can also have doubts about the ethical aspects of hiring workers who are often unaware of how this kind of task will affect their well-being. Being submersed in the cyber-Augean stables takes a toll on many---as examined by @roberts2014behind & @roberts2016commercial. Workers hired for such tasks are often low-status and low-wage, isolated and asked to keep what they've seen in secret under restrictive NDAs. This in turn makes the research in the area extremely difficult, since moderators are not allowed to talk about their work conditions or any other related subject. Those who decided to break the NDA are risking a penalty. 
Screening through the reported user-generated content is connected with exposure to violent and deeply disturbing materials, with child pornography, murders, or suicides as examples of the most extreme cases. This can lead to serious psychological damage, such as depression, or PTSD [@roberts2014behind]. Although there are certain initiatives being developed or introduced to reduce the emotional impact of the moderation, like stylistic alterations to a content (applying grayscale or blurring to images) [@karunakaran2019testing], workplace wellness programs, clinical support, or psychological training [@steiger2021psychological], none of the methods can eliminate the psychological distress completely. Some of the employees filed a lawsuit against Facebook and as a result, the company agreed to pay $52M in compensation for mental health issues developed during the job [@newton_facebook_2020]. Also as described by @parks2019dirty, the work is often performed under time pressure, reviewing 25K pieces of content per day. Spending on average three to five seconds on each image reported for moderation might not lead to the most thoughtful decisions and as shown @stepanikova2012racial, high time pressure can amplify human biases.
Taking into consideration that Facebook employs 15K moderators [@koetsier_report] and most likely more are needed to keep up with the growing amount of content, with the parallel considerations about the negative effects of content moderation on mental health, a collaboration between humans and machines in this area seems inevitable. 

# Pro-active and reactive moderation
There are different approaches when it comes to the moderation of online content. One can follow the workflow of reactive moderation, which happens once the content is published. Harmful messages can be either reported by the users of the platforms or automated methods and then sent for review to moderators. A set of actions can be then taken depending on the platform and their community guidelines---on the content or user level. A harmful message can be deleted, made invisible to other users, or certain profanities can be altered with special signs to censor them. Depending on the type and amount of infraction, a particular user can be warned, muted, shadowbanned, or banned from further participation in the community for a period of time. The weakness of the reactive method is that the damage is done. Whoever is the recipient of the abusive message has the chance to see it and potentially suffer [@hoff2009cyberbullying, @keipi2016online, @wachs2019associations]. 
Yet another weakness connected with relying solely on human reports is the content that is harmful but unreported by a receipted or any bystander. Although the exact scale of unreported content is not known, various self-report studies show that a lot of children, teens, or even adults do not report cyberbullying or harassment online [@noauthor_:game_nodate; @noauthor_free_nodate; @french_as_2021].

Yet another type of moderation can be distinguished as pro-active or pre-moderation. In pre-moderation, automated methods are either based on Artificial Intelligence or other less sophisticated tools (e.g. blacklists) and can screen the content before it gets published. If a type of harmful message gets detected, it can be removed before reaching the recipient. Due to the aforementioned dubious performance of state-of-the-art statistical methods, particularly low precision, they are rarely used autonomously. 

Pro-active moderation can be utilized using AI or other methods to promote socially positive engagement. Instead of or in the combination with punitive solutions such as privileges restriction, one can induce empathy or community norms with counter-speech. Counter speech as described by Dangerous Speech Project [@noauthor_counterspeech_2017] is "any direct response to hateful or harmful speech which seeks to undermine it". As examined by @munger2017tweetment, counter-speech can be effective in the reduction of racist tweets (although only in the condition in which a white male with high followers was approaching another white male). In a study conducted by @bilewicz2021artificial, a bot disguised as a Reddit user, equipped with normative and empathetic interventions, significantly decreased the amount of personal attacks generated on Reddit. In yet another study by @mivskolci2020countering, this technique was not effective in changing the behavior of the users (counter-speech here aimed at reducing the prejudice against Roma minority in Slovakia), but encouraged bystanders to express pro-Roma comments on specific Facebook posts. Counter-speech also has been shown to have the potential to increase civility online in studies conducted by @friess2021collective, @molina2018role, @han2018civility. 
...


# Collective Intelligence Approach to Counter-speech
Traditionally collective intelligence has been defined as "a group or a team’s combined capacity and capability to perform a wide variety of tasks and solve diverse problems" [@noauthor_collective_nodate]. In our paper and in the theoretical underpinnings of the experiment itself, we will be relying on a collective intelligence scope proposed by Nesta, an innovation foundation (https://www.nesta.org.uk), which focuses on a collaboration between human and machine intelligence to develop innovative solutions to social challenges. 

The main objective of the experiment was to test whether the level of verbal aggression (personal attacks) of a group of users' regularly attacking others on Reddit can be significantly decreased by community-driven, counter-speech interventions conducted by volunteers in partnership with Artificial Intelligence. Instead of using negative motivation system, the assumption was to test a positive one - convincing verbally violent users to refrain from using cyberviolence based on peer-pressure regulation and experiential learning of a positive set of norms and empathy. Algorithms developed for the detection of personal attacks were used to monitor the activity of experimental groups and notify volunteers about all attacks generated by its' members. Volunteers, after receiving a notification on Slack, could then react with a proper intervention. Such an approach served as a distributed bottom-up voluntary model of moderation based on collective intelligence---utilizing human + machine intelligence. 

In the end, what we were able to compare was the following: the effectiveness of the existing Reddit moderation system (predominantly grounded in a punitive authoritarian paradigm) versus the existing moderation system combined with collective intelligence---Artificial Intelligence supported with a crowd of volunteers---who introduced the element of  positive peer-pressure. 

\textbf{Empathetic interventions}
In the first treatment condition, volunteers were asked to send empathy-inducing messages focusing either on the target of verbal aggression (e.g. „Hey such words might hurt the other person”), stressing the common humanity aspect that we call share („We are all humans of flesh and blood”), or even infusing the intervention with the emphatic response to the attacker (Hey I understand your strong emotions…”). 
At the core of the empathetic interventions, we put forth the notion that goes back to David Hume and Adam Smith. The first one conceived empathy (at the time referred to as sympathy) as mirroring the emotional state of another person. In the academic psychological literature, similar phenomenon was distinguished and coined in the term emotional contagion
(https://www.sciencedirect.com/topics/psychology/emotional-contagion): "the process in which an observed behavioral change in one individual leads to the reflexive production of the same behavior by other individuals in close proximity, with the likely outcome of converging emotionally". 


For Adam Smith, sympathy consisted of visualizing how the sympathetic person would feel in the particular circumstances of the other --- thus here the process was based not so much on mirroring, but rather projecting my imagination of what it is like to be that person in a certain moment). Without further delving into the differentiation between those two, sympathy in both accounts is crucial in the constitution of human beings as social and moral creatures (https://plato.stanford.edu/entries/empathy/). It enables the emotional connection to others and concern for their well-being. The interventions were supposed to serve the role of empathetic response enablers --- reminding the person in front of the screen that there is a real human being on the other side.


In the psychological literature, various kinds of empathetic responses were distinguished - the aforementioned emotional contagion, affective/proper empathy, sympathy, personal distress or cognitive empathy ((https://plato.stanford.edu/entries/empathy/). During the experiment, we could not observe or measure whether the empathetic response indeed was evoked. The study was conducted in a digital setting --- interventions were most likely read in the private space, in front of personal computers or smartphones. Even if we would change the setting to a lab, differentiating and measuring empathy is not trivial if not impossible (to check). 


Also, interventions that stated that "such words might hurt the other person" have the underlying assumption that the receiver of the attack might be hurt, but in reality that might not even be the case. Intuitively, and following the argumentation of Thomas Nagel in “What is it like to be a bat?” one can only imagine what it is like --- but for me --- to be a bat or to be a receiver of the attack. To each observed or imagined experience there is an array of subjective quality to actually experiencing it and in this way receiving a particular message might be met with a unique reception by each conscious being. Also, full epistemic access to the mind and body of another is impossible. 


But even though such access is impossible and putting aside the broad spectrum of phenomena related to empathetic response, our goal was by utilizing interventions referring to empathy in various forms, changing the behavior of the attacker --- convincing him to refrain from using verbal aggression towards others. Whether the message indeed gave rise to empathy was not of importance --- one can imagine a hypothetical scenario of a successful intervention in which the attacker changed his behavior long-term and stopped attacking others, hopefully as a result of interventions, but the empathetic response was not even evoked. The behavioral change could be induced by other motivations --- e.g. unwillingness from the exclusion from the community. Thus the goal is not to evoke empathy, but through empathetic interventions --- substantially limit the use of verbal aggression among study group members.

\textbf{Normative interventions}
In the second treatment condition, volunteers were asked to send norms-inducing messages written on the grounds of deontology (e.g. „we have a duty to respect each other during the discussions"), virtue ethics (e.g. „capacity to be respectful in a heated discussion is a virtue and requires hard work”) or Reddit's community guideline (e.g. "Let's recall the reddiquette and adhere to the same standards of behavior online that you follow in real life").


Whatever the strategy was used, it was supposed to express the unacceptability of verbal aggression in a direct or non-direct way. A more direct approach referred to deontology in which „actions are good or bad according to a clear set of rules” (https://ethics.org.au/ethics-explainer-deontology/). The basic assumption behind those interventions is that there is something we ought to do or are morally required to do --- and such actions are the right actions. There are also actions we ought not to do --- and such actions are morally wrongful. This line of thinking can be traced back to Immanuel Kant who thought that all universal moral obligations can stem from categorical imperative: “act only in accordance with that maxim through which you can at the same time will that it become a universal law” (Groundwork of the Metaphysic of Morals).


In the psychological literature, Robert B. Cialdini distinguished three types of norms that he found to be effective in guiding human action: descriptive norms (focused on the perception of how the majority of people would behave), injunctive (based on the perception of how the majority of people would approve or disapprove certain conduct), or personal norms (based on the perception of how a particular person would approve or disapprove certain behavior) CITE: https://www.sciencedirect.com/science/article/abs/pii/S0065260108603305. Although interventions written on the grounds of duties and deontology cannot be reduced to any of the above, since following Hume's guillotine --- from the fact that the majority of people are respectful does not follow the normative statement of civility --- I hoped that expressing a universal normative statement will be even more powerful and potentially spawn an interpretation of social acceptability or unacceptability of certain behavior.


Yet other interventions in normative group were encouraged to be expressed in the light of virtue ethics and stressed the importance of adhering to or practicing certain virtues. Such a message was assumed to motivate the moral agent in the process of positive reinforcement and create a feeling of the desirability of certain behaviors, like kindness and civility. The core assumption here is that the virtue itself is not genetically inherited but is rather a potential or a disposition of a character that can be practice and mastered like a practical skill (even though as mentioned by Natasza Szutta in [cite her book about virtue ethics] virtues and practical skills share some fundamental differences). NS distinguished two types of virtues - affective and cognitive. Affective virtues encompass the emotions and feelings that play an important and positive role in morality and can act as a support in the course of becoming a virtuous man. A cognitive virtue relates to the intellectual aspect in which one knows how to act in certain situations and understands the rules of morally rightful actions (e.g. what is kindness and how a kind man acts).


Here, just as in the case of empathetic interventions, any attempts to measure whether the target of the intervention in the case of a positive outcome (behavioral change) indeed acted in a virtuous way. A virtue of kindness may manifest itself in particular behaviors but as such cannot be identified with the virtuous deed, as highlighted by Natasza Szutta. Although again, the goal of the experiment itself was to change the behavior of the attackers, and measuring whether particular messages contributed to more flourishing individuals in terms of virtue development and character creation lies beyond the scope of this work. Interventions could affect people either way - stimulate their moral reflections or limit their attacks due to consequentialist way of thinking and fear of potential outcomes (e.g. being excluded from the community). 


Additionally, Reddit created their social etiquette called "Rediquette" (https://www.reddithelp.com/hc/en-us/articles/205926439) which is "an informal expression of the values of many redditors, as written by redditors themselves." All users are encouraged to abide by it and moderators of communities (called subreddits) existing as a part of the platform have the authority to exclude its members based either on the basis of breaking the rules of the reddiquette or any other local rules imposed by specific subreddit. Volunteers were encouraged to refer to those norms as well, citing specific points, for instance, "Hey there, have you read the reddiquette? It says remember the human".

As we have seen, those two categories of interventions - normative and empathetic - encompass a whole variety of categories within. One might think that it would be useful to construct a more diverse set of treatment conditions in which we test each of the ethics or empathy differentiation separately - e.g. one for virtue ethics, deontology, perhaps utilitarian approach, empathy toward the receiver of the attack, empathy towards the attacker, so on and so forth. Additionally, different types of interventions could be tested and tailored depending on the spectrum on which one can be found in the foundations of moral reasoning, as proposed by Jonathan Haidt: Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression (J. Haidt, The Righteous Mind). Albeit for the sake of the sanity of our volunteers, we decided to create only two but broad categories and let volunteers decide based on the context of the conversation, how they want to respond to the attacker.


Cyberviolence was defined in this experiment as a personal attack - any kind of verbal harassment, insult, or threat directed against the interlocutor in a text-based conversation online. Those were detected using Samurai Labs' cyberviolence detection system. 

The following hypotheses were formulated: 

H1: If a group of human volunteers notified by an AI-based cyberviolence detection system about cyberviolence generated by the treatment group users (cyberviolence will be defined as a personal attack, harassment, or a threat targeted against an interlocutor) responds with counter-speech interventions, this will result in a decreased cyberviolence level for the whole group after the intervention period. 

H2: If two groups receive different types of interventions (empathy-based or normative), then the decrease in cyberviolence will be larger in the case of normative interventions in comparison to the empathy-based ones.

# Experimental Design and data collection
This was a 6-months field experiment in a digital setting conducted on a popular Q&A and news forum, Reddit (www.reddit.com). 
We formed treatment and control groups based on three main criteria:
\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
 \item During the intervention period, we have expected to have 20 active volunteers at any given time, each willing to conduct 10 interventions daily. Thus, we needed approximately 200 attacks daily generated by the treatment groups.
\item The identification of users who regularly attack others was necessary to measure the effect of interventions at the end.
\item The identification of users who were active during the whole preliminary monitoring period was necessary to minimize the risk of attrition during the study. 
 \end{enumerate}


User identification process:
\renewcommand{\labelenumii}{\Roman{enumii}}
 \begin{enumerate}
   \item First, we obtained 1 week of real-time (coming from 15-22nd of February 2020), unmoderated data from Reddit. The content was downloaded from the data stream provided by pushshift.io. 
   \item Samurai Labs Artificial Intelligence for personal attacks detection was applied to identify users who attacked others at least once within the aforementioned timeframe. This resulted in the identification of 93966 users. 
   \item We removed all accounts which we suspected not to be run by humans (AutoModerator and all users which had "bot" in the username string). This resulted in the removal of 388 users, thus 93578 were left on our list.
   \item Next, we removed users who generated only 1 personal attack during the week (leaving only those who attacked at least twice). As mentioned, the group of those regularly attacking others was crucial to measure the effects of the interventions. This step resulted in the removal of users below the third quartile (Q3). 20124 users were left in our group.
   \item Moving forward, we removed users who generated less than 14 comments in this week. We cared about most active users, and 2 comments per day per person on average seemed reasonable (not sure yet how to justify this - 14 comments is below 1st quartile (Q1:28, Q2: 63, Q3:126, mean=103)). This resulted in the removal of 2192 users, so 17932 were left.
   \item We discarded users whose personal attacks to all comments ratio was below 2\%. This means the inclusion in the sample of users above the 1st quartile. 4422 users were removed, leaving us with a group of 13510. 
   \item The next step of the process begun on March 9th, 2020, and lasted until May 5th, 2020 (9 weeks). During this period we have monitored the activity of the identified group of 13510 users and applied further selection criteria to make sure we select those who were regularly active and attacked other users. 
   \item The period of monitoring was divided into weeks. We have discarded those weeks during which technical difficulties occurred with the pushshift.io (resulting in missing data). Thus, we have taken into consideration only 6 full weeks for the period.
   \item Users who generated at least 1 attack during 5 out of 6 weeks were identified. First, we planned to restrict the list to only those users, who generate at least 1 attack during each week (6/6) but such restrictive criterion led to only 255 users left, which was not enough for the study. The less restrictive criterion (at least 1 attack generated during 5/6 weeks) resulted in 694 people. 
   \item Next, we calculated the daily average number of personal attacks generated by the group who met the above criteria (which resulted in 357 attacks per day, 1.94 attacks daily per person on average).
   \item Knowing that we need around 200 attacks/daily per treatment group (just enough for volunteers to keep up according to our assumption), we have randomly selected 195 users per each treatment group (normative and empathetic). The rest was delegated as a control group (304 users). 
   
 \end{enumerate}


The duration of the experiment, 6 months, was divided into three 2-months periods. The first two months served as a monitoring period to properly select groups and establish baselines. The next 2 months served as treatment period, during which groups received counter-speech comments from volunteers, in response to personal attacks detected by the Artificial Intelligence-based system. The last 2-months served as the post-treatment monitoring period to gather the data needed to evaluate the effectiveness of interventions. 



# Results



#  References {-}

\vspace{-3mm}





