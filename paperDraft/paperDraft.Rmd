---
title: "Nesta attacks study"
author: "Patrycja Tempska and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: yes
    df_print: kable
    keep_tex: yes
    includes:
      in_header: Rafal_latex6.sty
  html_document:
    df_print: paged
  word_document: default
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: ../references/attacks.bib
csl: ../references/apa-6th-edition.csl
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = '../')

#libraries used
library(ggplot2)
library(ggthemes)
library(gridExtra)

```



\tableofcontents

# Section Abstract
This article describes an experimental intervention study based in a naturalistic, digital setting (Q&A forum - Reddit), utilizing a collective intelligence approach to content moderation and reduction of the level of verbal aggression among a selected group of Reddit users who regularly attack other community members. Collective Intelligence in this sense means exploring the collaboration between human and machine intelligence to develop solutions to social challenges. Artificial Intelligence was used to detect verbal aggression (personal attacks) and notify human volunteers about attacks. Volunteers after receiving notifications employed interventions based on norm or empathy promotion. We find that only those who were sanctioned with norms-inducing interventions had their personal attacks' user significantly decreased. 

\vspace{1mm}
\footnotesize
```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
2+2 #use this formatting for chunks
```
\normalsize


# Introduction
Although much effort has been made in order to tackle the problem of verbal aggression and harassment online, looking at various reports and surveys, it remains a common hindrance for people engaging with social media in their everyday lives. The situation got exacerbated amidst the COVID19 pandemic, during which a majority of our social life moved to cyberspace. During this shift, there was an increase in cyberbullying attitudes and perpetration (@barlett2021comparing), 90% increase in public reports of illegal online content\footnote{\href {https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}{https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}}, including 114% increase in non-consensual sharing of intimate images, 30% increase in cyberbullying, as well as 40% of increase in adults reporting online harassment. According to a report conducted by company L1ght \footnote{\href {https://l1ght.com/Toxicity_during_coronavirus_Report-L1ght.pdf}{https://l1ght.com/Toxicity_during_coronavirus_Report-L1ght.pdf}}, hate speech directed towards China and the Chinese went up by 900% on Twitter. Gaming platforms were in the spotlight as well, with a 40% increase in toxicity on Discord. 

But alongside the growing need for even more efficient and proactive moderation, the capacity to execute it did not go hand in hand, forcing companies and policymakers to rethink the current model of moderation processes and workforce. Due to the COVID19 restrictions including social distancing, a lot of those serving the role of moderators had to be sent home\footnote{\href {https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}{https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}} without the ability to work remotely because of the constraints affiliated with restrictive non-disclosure agreements (NDA) among others. Curtailing the moderators' workforce was accompanied by more agency given to algorithms and AI-based moderation. Those changes, as argued by @gerrard2020covid19, can be seen as a serious red flag in terms of safety for all users on online platforms. 

# Automated versus Human-based Moderation
The hindrances and threats that go along with the Artificial Intelligence-based methods for moderation have been widely debated, with the most critical discussions revolving around technology performance (@macavaney2019hate, @schmidt2017survey). State-of-the-art solutions are mostly governed by statistical methods including deep learning and machine learning (@lecun2015deep, @sejnowski2020unreasonable, @jordan2015machine). Their performance is inherently tied to the amount of data being fed to the system and the quality of its annotation. At different stages of the process, from datasets gathering and preparation, annotation to the training or algorithms themselves, biases seem to be omnipresent (@binns2017like, @geva2019we @mehrabi2021survey). Users' of online services are also creative in their strategies to circumvent automated content moderation systems and as shown by @grondahl2018all, current techniques are vulnerable to the most common evasion attacks like word changes (inseting typos and leetspeak), word-boundary changes (inserting or removing whitespace), or word appending (appending common or non-hateful words like "love"). Generalisability of the models - an ability to perform well on datasets coming from sources other than the one used for training are an important shortcoming as well (@yin2021towards, @swamy2019studying, @rosa2019automatic). As shown by @wu2019errudite, @lipton2019troubling, and @musgrave2020metric in practice, creations of models often lacks thorough error analysis and legitimate experimental methodology, which can result in non-reproducibility. This is also connected with a potential lack of thorough understanding of the limitations of the models and spurious conclusions being made to a wider public. Specifically, @@lipton2019troubling distinguishes four dysfunctional patterns occurring in the current research paradigm in the industry and academia alike. First, the inability to draw a clear distinction between speculation and explanation, with the first one often being disguised as the second. Second, inability for successful identification of the sources of empirical gains (whether it was problem formulation, optimization of the heuristics, data-preprocessing, hyperparameter tuning, or perhaps yet another aspect). Third, "mathiness" - the use obscure language and often covering weak argumentation with the alluring but often apparent depth of technical jargon. Last but not least - misuse of language. This includes suggestive definitions without proper explanation of what they mean in the context (e.g. inflating good performance in simple NLP tasks to human-level natural understanding), overloading the papers with technical terminology, or suitcase words (those words that can encompass a variety of meanings, e.g. consciousness).

Yet another obstacle in the process is the lack of gold standard in dataset creation and taxonomies of abusive language being used for instance in the process of annotating different datasets. Frequently people obtain data from various sources and do not follow any universally used instructions when it comes to its annotation, leading to discrepancies between various datasets being tagged within one domain (e.g. hate speech). Lack of expert annotators and proper annotation criteria and instructions are also widespread, with the common practice hiring untrained workers from Mechanical Turk or other crowdsourcing platforms.

Although there are some initiatives developed in response, most notably, functional tests for Hate Speech Detection Models created by @rottger2020hatecheck, or the Online Safety Data Initiative (OSDI) LINK https://onlinesafetydata.blog.gov.uk/about-us/, focused on projects related to improving access to data, standardizing the description of online harms, as well as creating tools and benchmarks for evaluation of technologies focused on safety, much effort must be made before wider adoption of such solutions comes into force.  

At the same time, only automated methods can scan through the massive amount of content being generated every day on different platforms. On Facebook, there are more than 3B comments and likes daily (https://martech.org/facebook-3-2-billion-likes-comments-every-day/), 500M tweets are sent daily on Twitter (https://www.oberlo.com/blog/twitter-statistics), and over 2B comments made by users of Reddit in 2020 (https://old.reddit.com/r/blog/comments/k967mm/reddit_in_2020/) which is almost 3M comments made daily. With this amount of content, it's either impossible or extremely costly to scale the moderation workforce. One can also have doubts about the ethical aspects of hiring workers who are often unaware of how this kind of task will affect their well-being. Being submersed in the cyber-Augean stables takes a toll on many people. As examined by @roberts2014behind & @roberts2016commercial, workers who are hired for such tasks are often low-status and low-wage, isolated, and asked to keep what they've seen in secret under restrictive NDAs. Screening through the reported user-generated content is connected with exposure to violent and deeply disturbing materials, with child pornography, murders, or suicides as examples of the most extreme cases. This can lead to serious psychological damage, like depression, or PTSD (@roberts2014behind). Some of the employees filed a lawsuit against Facebook and as a result, the company agreed to pay $52M in compensation for mental health issues developed during the job (https://www.theverge.com/2020/5/12/21255870/facebook-content-moderator-settlement-scola-ptsd-mental-health).
Taking into consideration that Facebook employs 15K moderators (https://www.forbes.com/sites/johnkoetsier/2020/06/09/300000-facebook-content-moderation-mistakes-daily-report-says/?sh=6c6bdbbc54d0) and most likely more are needed to keep up with the growing amount of content, with the parallel considerations about the negative effects of content moderation on mental health, a collaboration between humans and machines in this area seems inevitable. There is yet another aspect here - what moderators deal with is mostly content reported by humans. And as shown in various studies and reports, a lot of children, teens or even adults do not report cyberbullying or harassment online (LINKS: https://www.ctvnews.ca/canada/as-the-pandemic-forces-us-online-lgbtq2s-teens-deal-with-cyberbullying-1.5430945 ; https://www.adl.org/free-to-play-2020 ; https://www.ditchthelabel.org/wp-content/uploads/2017/05/InGameAbuse.pdf). 

# Pro-active and reactive moderation

# Collective Intelligence Approach to Counter-speech

# Experimental Design
This was a 6-months field experiment in a digital setting conducted on a popular Q&A and news forum, Reddit (www.reddit.com). 
We formed treatment and control groups based on three main criteria:
1. During the intervention period, we have expected to have 20 active volunteers at any given time, each willing to conduct 10 interventions daily. Thus, we needed approximately 200 attacks daily generated by the treatment groups.
2. Recruitment of users who regularly attack others was necessary to measure the effect of interventions at the end.
3. Recruitment of users who were active during the whole preliminary monitoring period was necessary to minimize the risk of attrition during the study. 
First, we spent 9 weeks for preliminary Reddit's monitoring period to recruit only those users who generate at least one personal attack per week and were active during the whole period of the monitoring. As mentioned, sustained activity was crucial to minimize the risk users becoming inactive during the course of the study. Next, we have calculated the daily average number of personal attacks generated by the group who met the above criteria (which resulted in 357 attacks per day - 1.94 attacks daily per person on average). Knowing that we need around 200 attacks daily (just enough so our volunteers can keep up with the volume) we have randomly selected 390 people for our study groups (195 per group). The rest (304 people) were selected as our control group. 

Three groups were formed - two treatment groups (195 people in each) and one control group (304 people). The first treatment group received counter-speech interventions based on normative influence, while the second one received interventions based on empathetic influence. Users in the control group did not receive any intervention. 

The duration of the experiment, 6 months, was divided into three 2-months periods. The first two months served as a monitoring period to properly select groups and establish baselines. The next 2 months served as treatment period, during which groups received counter-speech comments from volunteers, in response to personal attacks detected by the Artificial Intelligence-based system. The last 2-months served as the post-treatment monitoring period to gather the data needed to evaluate the effectiveness of interventions. 


Normative interventions can refer either to general social norms of civility and respect, community standards, a particular subreddit rule, or a descriptive norm among others.
E.g. Insulting others is against Reddit's policy. 
Hey there, we do not call each other like that here.
Empathetic interventions can refer to the emotional state of the recipient or the sender of the attack or both. They are designed to evoke an empathetic response. 
E.g. There is a human being on the other side who might be hurt by your words.
I see you are frustrated but remember the human.

Cyberviolence was defined in this experiment as a personal attack - any kind of verbal harassment, insult, or threat directed against the interlocutor in a text-based conversation online. Those were detected using Samurai Labs' cyberviolence detection system. 

The following hypotheses were formulated: 

H1: If a group of human volunteers notified by an AI-based cyberviolence detection system about cyberviolence generated by the treatment group users (cyberviolence will be defined as a personal attack, harassment, or a threat targeted against an interlocutor) responds with counter-speech interventions, this will result in a decreased cyberviolence level for the whole group after the intervention period. 

H2: If two groups receive different types of interventions (empathy-based or normative), then the decrease in cyberviolence will be larger in the case of normative interventions in comparison to the empathy-based ones.


# Results





#  References {-}

\vspace{-3mm}





