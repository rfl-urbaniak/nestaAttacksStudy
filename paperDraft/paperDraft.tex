% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  10pt,
  dvipsnames,enabledeprecatedfontcommands]{scrartcl}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Nesta attacks study},
  pdfauthor={Patrycja Tempska and Rafal Urbaniak},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\documentclass{article}

% %packages
 \usepackage{booktabs}

\usepackage{multirow}

\usepackage{graphicx}
\usepackage{longtable}
\usepackage{ragged2e}
\usepackage{etex}
%\usepackage{yfonts}
\usepackage{marvosym}
\usepackage[notextcomp]{kpfonts}
\usepackage{nicefrac}
\newcommand*{\QED}{\hfill \footnotesize {\sc Q.e.d.}}
\usepackage{floatrow}

\usepackage[textsize=footnotesize]{todonotes}
%\linespread{1.5}


\setlength{\parindent}{10pt}
\setlength{\parskip}{1pt}


%language
\usepackage{times}
\usepackage{t1enc}
%\usepackage[utf8x]{inputenc}
%\usepackage[polish]{babel}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx}
\usepackage[scaled=0.88]{helvet}


%AMS
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{geometry}
 \geometry{a4paper,left=35mm,top=20mm,}


%environments
\newtheorem{fact}{Fact}



%abbreviations
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\n}{\neg}
\newcommand{\et}{\wedge}
\newcommand{\jt}{\rightarrow}
\newcommand{\ko}[1]{\forall  #1\,}
\newcommand{\ro}{\leftrightarrow}
\newcommand{\exi}[1]{\exists\, {_{#1}}}
\newcommand{\pr}[1]{\mathsf{P}(#1)}
\newcommand{\cost}{\mathsf{cost}}


\newcommand{\odds}{\mathsf{Odds}}
\newcommand{\ind}{\mathsf{Ind}}
\newcommand{\nf}[2]{\nicefrac{#1\,}{#2}}
\newcommand{\R}[1]{\texttt{#1}}
\newcommand{\prr}[1]{\mbox{$\mathtt{P}_{prior}(#1)$}}
\newcommand{\prp}[1]{\mbox{$\mathtt{P}_{posterior}(#1)$}}



\newtheorem{q}{\color{blue}Question}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}



%technical intermezzo
%---------------------

\newcommand{\intermezzoa}{
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}



	\tiny{\sc Optional Content Starts}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}\nopagebreak 
	}


\newcommand{\intermezzob}{\nopagebreak 
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}

	\tiny{\sc Optional Content Ends}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}
	}
%--------------------






















\newtheorem*{reply*}{Reply}
\usepackage{enumitem}
\newcommand{\question}[1]{\begin{enumerate}[resume,leftmargin=0cm,labelsep=0cm,align=left]
\item #1
\end{enumerate}}

\usepackage{float}

% \setbeamertemplate{blocks}[rounded][shadow=true]
% \setbeamertemplate{itemize items}[ball]
% \AtBeginPart{}
% \AtBeginSection{}
% \AtBeginSubsection{}
% \AtBeginSubsubsection{}
% \setlength{\emergencystretch}{0em}
% \setlength{\parskip}{0pt}






\usepackage[authoryear]{natbib}

%\bibliographystyle{apalike}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Nesta attacks study}
\author{Patrycja Tempska and Rafal Urbaniak}
\date{}

\begin{document}
\maketitle

\tableofcontents

\hypertarget{section-abstract}{%
\section{Section Abstract}\label{section-abstract}}

This article describes an experimental intervention study based in a
naturalistic, digital setting (Q\&A forum - Reddit), utilizing a
collective intelligence approach to content moderation and reduction of
the level of verbal aggression among a selected group of Reddit users
who regularly attack other community members. Collective Intelligence in
this sense means exploring the collaboration between human and machine
intelligence to develop solutions to social challenges. Artificial
Intelligence was used to detect verbal aggression (personal attacks) and
notify human volunteers about attacks. Volunteers after receiving
notifications employed interventions based on norm or empathy promotion.
We find that only those who were sanctioned with norms-inducing
interventions had their personal attacks' level significantly decreased.

\vspace{1mm}
\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\SpecialCharTok{+}\DecValTok{2} \CommentTok{\#use this formatting for chunks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\normalsize

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Although much effort has been made in order to tackle the problem of
verbal aggression and harassment online, looking at various reports and
surveys \todo{refs}, it remains a common hindrance for people engaging
with social media in their everyday lives. The problem got exacerbated
in the midst of the COVID19 pandemic \todo{is it less coloquial now?},
during which a majority of our social life moved to cyberspace. During
this shift, there was an increase in cyberbullying attitudes and
perpetration (Barlett, Simmers, Roth, \& Gentile (2021)), 90\% increase
in public reports of illegal online content Grant (2021), including
114\% increase in non-consensual sharing of intimate images, 30\%
increase in cyberbullying, as well as 40\% of increase in adults
reporting online
harassment.\footnote{The increase is reported as a relative change between the year 2019 and 2020.}
According to a report conducted by company L1ght (L1ght, 2020), hate
speech directed towards China and the Chinese went up by 900\% on
Twitter. Gaming platforms were in the spotlight as well, with a 40\%
increase in toxicity on Discord
\footnote{In this industry report, the increase is reported as a relative change between the year 2019 and 2020 as well, with no absolute indicators.}.

But alongside the growing need for even more efficient and proactive
moderation, the capacity to execute it did not go hand in hand, forcing
companies and policymakers to rethink the current model of moderation
processes and workforce. Due to the COVID19 restrictions including
social distancing, a lot of those serving the role of moderators had to
be sent home (Bhattacharya, 2021) without the ability to work remotely
because of the constraints affiliated with restrictive non-disclosure
agreements (NDA) among others. Curtailing the moderators' workforce was
accompanied by more agency given to algorithms and AI-based moderation.
Those changes, as argued by Gerrard (2020), can be seen as a serious red
flag in terms of safety for all users on online platforms.

\hypertarget{automated-versus-human-based-moderation}{%
\section{Automated versus Human-based
Moderation}\label{automated-versus-human-based-moderation}}

The hindrances and threats that go along with the Artificial
Intelligence-based methods for moderation have been widely debated, with
the most critical discussions revolving around technology performance
(MacAvaney et al., 2019; Schmidt \& Wiegand, 2017). State-of-the-art
solutions are mostly governed by statistical methods including deep
learning and machine learning (Jordan \& Mitchell, 2015; LeCun, Bengio,
\& Hinton, 2015; Sejnowski, 2020). Their performance is inherently tied
to the amount of data being fed to the system and the quality of its
annotation. \todo{This deserves a developed paragraph with examples}At
different stages of the process, from data gathering and preparation,
annotation to the training or algorithms themselves, biases seem to be
omnipresent Mehrabi, Morstatter, Saxena, Lerman, \& Galstyan (2021).

\todo{This deserves a developed paragraph with examples}Users' of online
services are also creative in their strategies to circumvent automated
content moderation systems and as shown by Gröndahl, Pajola, Juuti,
Conti, \& Asokan (2018), current techniques are vulnerable to the most
common evasion attacks like word changes (inseting typos and leetspeak),
word-boundary changes (inserting or removing whitespace), or word
appending (appending common or non-hateful words like ``love'').

\todo{This deserves a developed paragraph with examples}Lack of
generalisability of the models---an ability to perform well on datasets
coming from sources other than the one used for training is a serious
shortcoming as well (Rosa et al., 2019; Swamy, Jamatia, \& Gambäck,
2019; Yin \& Zubiaga, 2021). As shown by Wu, Ribeiro, Heer, \& Weld
(2019), Lipton \& Steinhardt (2019), and Musgrave, Belongie, \& Lim
(2020) in practice, the development of such models often lacks thorough
error analysis and legitimate experimental methodology, which can result
in non-reproducibility. This is also connected with a potential lack of
thorough understanding of the limitations of the models and spurious
conclusions being made to a wider public. Specifically, @Lipton \&
Steinhardt (2019) distinguishes four dysfunctional patterns occurring in
the current research paradigm in the industry and academia alike. First,
the inability to draw a clear distinction between speculation and
explanation, with the first one often being disguised as the
second\todo{give an example}. Second, the inability of successful
identification of the sources of performance improvement (whether it was
problem formulation, optimization of the heuristics, data-preprocessing,
hyperparameter tuning, or perhaps yet another
aspect)\todo{give an example}. Third, ``mathiness''---the use of obscure
language and often covering weak argumentation with the alluring but
often apparent depth of technical jargon\todo{Give an example}. Last but
not least---misuse of language\todo{Give an example}. This includes
suggestive definitions without proper explanation of what they mean in
the context (e.g.~inflating good performance in simple NLP tasks to
human-level understanding), overloading the papers with technical
terminology, or suitcase words (words that can encompass a variety of
meanings, e.g.~consciousness).

Yet another obstacle in the process is the lack of gold standard in
dataset creation and taxonomies of abusive language being used for
instance in the process of annotating different datasets. Frequently
people obtain data from various sources and do not follow any
universally used instructions when it comes to annotation, leading to
discrepancies between various datasets being tagged within one domain
(e.g.~hate speech). Lack of expert annotators and proper annotation
criteria and instructions are also widespread, with the common practice
of hiring untrained workers from Mechanical Turk or other crowdsourcing
platforms.

Although there are some initiatives developed in response, most notably,
functional tests for Hate Speech Detection Models created by Röttger et
al. (2020), or the Online Safety Data Initiative (OSDI) (\emph{About the
Online Safety Data Initiative}, n.d.), focused on projects related to
improving access to data, standardizing the description of online harms,
as well as creating tools and benchmarks for evaluation of technologies
focused on safety, much effort must be made before wider adoption of
such solutions comes into force.

At the same time, only automated methods can scan through the massive
amount of content being generated every day on different platforms. On
Facebook, there are more than 3B comments and likes daily
({``Facebook,''} 2012), 500M tweets are sent daily on Twitter (\emph{10
{Twitter} {Statistics} {Every} {Marketer} {Should} {Know} in 2021
{[}{Infographic}{]}}, 2021), and over 2B comments made by users of
Reddit in 2020 ({``Reddit in 2020,''} 2020) which is almost 3M comments
made daily. With this amount of content, it's either impossible or
extremely costly to scale the moderation workforce. One can also have
doubts about the ethical aspects of hiring workers who are often unaware
of how this kind of task will affect their well-being. Being submersed
in the cyber-Augean stables takes a toll on many---as examined by
Roberts (2014) \& Roberts (2016). Workers hired for such tasks are often
low-status and low-wage, isolated and asked to keep what they've seen in
secret under restrictive NDAs. This in turn makes the research in the
area extremely difficult, since moderators are not allowed to talk about
their work conditions or any other related subject. Those who decided to
break the NDA are risking a penalty. Screening through the reported
user-generated content is connected with exposure to violent and deeply
disturbing materials, with child pornography, murders, or suicides as
examples of the most extreme cases. This can lead to serious
psychological damage, such as depression, or PTSD (Roberts, 2014). Some
of the employees filed a lawsuit against Facebook and as a result, the
company agreed to pay \$52M in compensation for mental health issues
developed during the job (Newton, 2020). Taking into consideration that
Facebook employs 15K moderators (Koetsier, 2020) and most likely more
are needed to keep up with the growing amount of content, with the
parallel considerations about the negative effects of content moderation
on mental health, a collaboration between humans and machines in this
area seems inevitable.

\hypertarget{pro-active-and-reactive-moderation}{%
\section{Pro-active and reactive
moderation}\label{pro-active-and-reactive-moderation}}

There are different approaches when it comes to the moderation of online
content. One can follow the workflow of reactive moderation, which
happens once the content is published. Harmful messages can be either
reported by the users of the platforms or automated methods and then
sent for review to moderators. A set of actions can be then taken
depending on the platform and their community guidelines---on the
content or user level. A harmful message can be deleted, made invisible
to other users, or certain profanities can be altered with special signs
to censor them. Depending on the type and amount of infraction, a
particular user can be warned, muted or shadowbanned, or banned from
further participation in the community for a period of time. The
weakness of the reactive method is that the damage is done. Whoever is
the recipient of the abusive message has the chance to see it and
potentially suffer Wachs et al. (2019). Yet another weakness connected
with relying solely on human reports is the content that is harmful but
unreported by a receipted or any bystander. Although the exact scale of
unreported content is not known, various self-report studies show that a
lot of children, teens, or even adults do not report cyberbullying or
harassment online ({``Free to {Play}?''} 2020; French, 2021; {``In,''}
2017).

Yet another type of moderation can be distinguished as pro-active or
pre-moderation. In pre-moderation, automated methods are either based on
Artificial Intelligence or other less sophisticated tools
(e.g.~blacklists) and can screen the content before it gets published.
If a type of harmful message gets detected, it can be removed before
reaching the recipient. Due to the aforementioned dubious performance of
state-of-the-art statistical methods, particularly low precision, they
are rarely used autonomously.

Pro-active moderation can be utilized using AI or other methods to
promote socially positive engagement. Instead of or in the combination
with punitive solutions such as privileges restriction, one can induce
empathy or community norms with counter-speech. Counter speech as
described by Dangerous Speech Project ({``Counterspeech {{}} {Dangerous}
{Speech} {Project},''} 2017) is ``any direct response to hateful or
harmful speech which seeks to undermine it.'' As examined by Munger
(2017), counter-speech can be effective in the reduction of racist
tweets (although only in the condition in which a white male with high
followers was approaching another white male). In a study conducted by
Bilewicz et al. (2021), a bot disguised as a Reddit user, equipped with
normative and empathetic interventions, significantly decreased the
amount of personal attacks generated on Reddit. In yet another study by
Miškolci, Kováčová, \& Rigová (2020), this technique was not effective
in changing the behavior of the users (counter-speech here aimed at
reducing the prejudice against Roma minority in Slovakia), but
encouraged bystanders to express pro-Roma comments on specific Facebook
posts. Counter-speech also has been shown to have the potential to
increase civility online in studies conducted by Friess, Ziegele, \&
Heinbach (2021), Molina \& Jennings (2018), Han, Brazeal, \& Pennington
(2018). \ldots{}

\hypertarget{collective-intelligence-approach-to-counter-speech}{%
\section{Collective Intelligence Approach to
Counter-speech}\label{collective-intelligence-approach-to-counter-speech}}

\hypertarget{experimental-design-and-data-collection}{%
\section{Experimental Design and data
collection}\label{experimental-design-and-data-collection}}

This was a 6-months field experiment in a digital setting conducted on a
popular Q\&A and news forum, Reddit (www.reddit.com). We formed
treatment and control groups based on three main criteria: 1. During the
intervention period, we have expected to have 20 active volunteers at
any given time, each willing to conduct 10 interventions daily. Thus, we
needed approximately 200 attacks daily generated by the treatment
groups. 2. Identification of users who regularly attack others was
necessary to measure the effect of interventions at the end. 3.
Identification of users who were active during the whole preliminary
monitoring period was necessary to minimize the risk of attrition during
the study. First, we spent 9 weeks for preliminary monitoring period to
recruit only those users who generate at least one personal attack per
week and were active during the whole period of the monitoring. As
mentioned, sustained activity was crucial to minimize the risk of users
becoming inactive during the course of the study. Next, we calculated
the daily average number of personal attacks generated by the group who
met the above criteria (which resulted in 357 attacks per day, 1.94
attacks daily per person on average). Knowing that we need around 200
attacks daily (just enough so our volunteers can keep up with the
volume) we have randomly selected 390 people for our study groups (195
per intervention type). The rest (304 people) were selected as our
control group. The first treatment group received normative
counter-speech interventions, while the second one received empathetic
interventions. Users in the control group did not receive any
intervention.

The duration of the experiment, 6 months, was divided into three
2-months periods. The first two months served as a monitoring period to
properly select groups and establish baselines. The next 2 months served
as treatment period, during which groups received counter-speech
comments from volunteers, in response to personal attacks detected by
the Artificial Intelligence-based system. The last 2-months served as
the post-treatment monitoring period to gather the data needed to
evaluate the effectiveness of interventions.

Normative interventions can refer either to general social norms of
civility and respect, community standards, a particular subreddit rule,
or a descriptive norm among others. E.g. Insulting others is against
Reddit's policy. Hey there, we do not call each other like that here.
Empathetic interventions can refer to the emotional state of the
recipient or the sender of the attack or both. They are designed to
evoke an empathetic response. E.g. There is a human being on the other
side who might be hurt by your words. I see you are frustrated but
remember the human.

Cyberviolence was defined in this experiment as a personal attack - any
kind of verbal harassment, insult, or threat directed against the
interlocutor in a text-based conversation online. Those were detected
using Samurai Labs' cyberviolence detection system.

The following hypotheses were formulated:

H1: If a group of human volunteers notified by an AI-based cyberviolence
detection system about cyberviolence generated by the treatment group
users (cyberviolence will be defined as a personal attack, harassment,
or a threat targeted against an interlocutor) responds with
counter-speech interventions, this will result in a decreased
cyberviolence level for the whole group after the intervention period.

H2: If two groups receive different types of interventions
(empathy-based or normative), then the decrease in cyberviolence will be
larger in the case of normative interventions in comparison to the
empathy-based ones.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\vspace{-3mm}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-noauthor_10_2021}{}%
\emph{10 {Twitter} {Statistics} {Every} {Marketer} {Should} {Know} in
2021 {[}{Infographic}{]}}. (2021). Retrieved from
\url{https://www.oberlo.com/blog/twitter-statistics}

\leavevmode\hypertarget{ref-onlinesafetydata}{}%
\emph{About the online safety data initiative}. (n.d.).

\leavevmode\hypertarget{ref-barlett2021comparing}{}%
Barlett, C. P., Simmers, M. M., Roth, B., \& Gentile, D. (2021).
Comparing cyberbullying prevalence and process before and during the
COVID-19 pandemic. \emph{The Journal of Social Psychology}, 1--11.

\leavevmode\hypertarget{ref-bhattacharya}{}%
Bhattacharya, A. (2021). How {Covid}-19 lockdowns weakened {Facebook}'s
content moderation algorithms. Retrieved from
\url{https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}

\leavevmode\hypertarget{ref-bilewicz2021artificial}{}%
Bilewicz, M., Tempska, P., Leliwa, G., Dowgiałło, M., Tańska, M.,
Urbaniak, R., \& Wroczyński, M. (2021). Artificial intelligence against
hate: Intervention reducing verbal aggression in the social network
environment. \emph{Aggressive Behavior}, \emph{47}(3), 260--266.

\leavevmode\hypertarget{ref-binns2017like}{}%
Binns, R., Veale, M., Van Kleek, M., \& Shadbolt, N. (2017). Like
trainer, like bot? Inheritance of bias in algorithmic content
moderation. \emph{International Conference on Social Informatics},
405--415. Springer.

\leavevmode\hypertarget{ref-noauthor_counterspeech_2017}{}%
Counterspeech {{}} {Dangerous} {Speech} {Project}. (2017). Retrieved
from \url{https://dangerousspeech.org/counterspeech/}

\leavevmode\hypertarget{ref-noauthor_facebook:_2012}{}%
Facebook: 3.2 {Billion} {Likes} \& {Comments} {Every} {Day}. (2012).
Retrieved from
\url{https://martech.org/facebook-3-2-billion-likes-comments-every-day/}

\leavevmode\hypertarget{ref-noauthor_free_nodate}{}%
Free to {Play}? {Hate}, {Harassment} and {Positive} {Social}
{Experience} in {Online} {Games} 2020. (2020). Retrieved from
\url{https://www.adl.org/free-to-play-2020}

\leavevmode\hypertarget{ref-french_as_2021}{}%
French, C. (2021). As the pandemic forces us online, {LGBTQ2S}+ teens
deal with cyberbullying. Retrieved from
\url{https://www.ctvnews.ca/canada/as-the-pandemic-forces-us-online-lgbtq2s-teens-deal-with-cyberbullying-1.5430945}

\leavevmode\hypertarget{ref-friess2021collective}{}%
Friess, D., Ziegele, M., \& Heinbach, D. (2021). Collective civic
moderation for deliberation? Exploring the links between citizens'
organized engagement in comment sections and the deliberative quality of
online discussions. \emph{Political Communication}, 1--23.

\leavevmode\hypertarget{ref-gerrard2020covid19}{}%
Gerrard, Y. (2020). \textless? covid19?\textgreater{} The COVID-19
mental health content moderation conundrum. \emph{Social Media+
Society}, \emph{6}(3), 2056305120948186.

\leavevmode\hypertarget{ref-geva2019we}{}%
Geva, M., Goldberg, Y., \& Berant, J. (2019). Are we modeling the task
or the annotator? An investigation of annotator bias in natural language
understanding datasets. \emph{arXiv Preprint arXiv:1908.07898}.

\leavevmode\hypertarget{ref-grant_2021}{}%
Grant, J. (2021). Australia's eSafety commissioner targets abuse online
as covid-19 supercharges cyberbullying \textbar{} the strategist.
Retrieved from
\url{https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}

\leavevmode\hypertarget{ref-grondahl2018all}{}%
Gröndahl, T., Pajola, L., Juuti, M., Conti, M., \& Asokan, N. (2018).
All you need is" love" evading hate speech detection. \emph{Proceedings
of the 11th ACM Workshop on Artificial Intelligence and Security},
2--12.

\leavevmode\hypertarget{ref-han2018civility}{}%
Han, S.-H., Brazeal, L. M., \& Pennington, N. (2018). Is civility
contagious? Examining the impact of modeling in online political
discussions. \emph{Social Media+ Society}, \emph{4}(3),
2056305118793404.

\leavevmode\hypertarget{ref-hoff2009cyberbullying}{}%
Hoff, D. L., \& Mitchell, S. N. (2009). Cyberbullying: Causes, effects,
and remedies. \emph{Journal of Educational Administration}.

\leavevmode\hypertarget{ref-noauthor_:game_nodate}{}%
In:{Game} {Abuse}. (2017). Retrieved from
\url{https://www.ditchthelabel.org/research-papers/ingame-abuse/}

\leavevmode\hypertarget{ref-jordan2015machine}{}%
Jordan, M. I., \& Mitchell, T. M. (2015). Machine learning: Trends,
perspectives, and prospects. \emph{Science}, \emph{349}(6245), 255--260.

\leavevmode\hypertarget{ref-keipi2016online}{}%
Keipi, T., Näsi, M., Oksanen, A., \& Räsänen, P. (2016). \emph{Online
hate and harmful content: Cross-national perspectives}. Taylor \&
Francis.

\leavevmode\hypertarget{ref-koetsier_report}{}%
Koetsier, J. (2020). Report: {Facebook} {Makes} 300,000 {Content}
{Moderation} {Mistakes} {Every} {Day}. Retrieved from
\url{https://www.forbes.com/sites/johnkoetsier/2020/06/09/300000-facebook-content-moderation-mistakes-daily-report-says/}

\leavevmode\hypertarget{ref-noauthor_l1ght_2020}{}%
L1ght. (2020). L1ght releases groundbreaking report on corona-related
hate speech and online toxicity. Retrieved from
\url{https://l1ght.com/l1ght-releases-groundbreaking-report-on-corona-related-hate-speech-and-online-toxicity/}

\leavevmode\hypertarget{ref-lecun2015deep}{}%
LeCun, Y., Bengio, Y., \& Hinton, G. (2015). Deep learning.
\emph{Nature}, \emph{521}(7553), 436--444.

\leavevmode\hypertarget{ref-lipton2019troubling}{}%
Lipton, Z. C., \& Steinhardt, J. (2019). Troubling trends in machine
learning scholarship: Some ML papers suffer from flaws that could
mislead the public and stymie future research. \emph{Queue},
\emph{17}(1), 45--77.

\leavevmode\hypertarget{ref-macavaney2019hate}{}%
MacAvaney, S., Yao, H.-R., Yang, E., Russell, K., Goharian, N., \&
Frieder, O. (2019). Hate speech detection: Challenges and solutions.
\emph{PloS One}, \emph{14}(8), e0221152.

\leavevmode\hypertarget{ref-mehrabi2021survey}{}%
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., \& Galstyan, A.
(2021). A survey on bias and fairness in machine learning. \emph{ACM
Computing Surveys (CSUR)}, \emph{54}(6), 1--35.

\leavevmode\hypertarget{ref-mivskolci2020countering}{}%
Miškolci, J., Kováčová, L., \& Rigová, E. (2020). Countering hate speech
on facebook: The case of the roma minority in slovakia. \emph{Social
Science Computer Review}, \emph{38}(2), 128--146.

\leavevmode\hypertarget{ref-molina2018role}{}%
Molina, R. G., \& Jennings, F. J. (2018). The role of civility and
metacommunication in facebook discussions. \emph{Communication Studies},
\emph{69}(1), 42--66.

\leavevmode\hypertarget{ref-munger2017tweetment}{}%
Munger, K. (2017). Tweetment effects on the tweeted: Experimentally
reducing racist harassment. \emph{Political Behavior}, \emph{39}(3),
629--649.

\leavevmode\hypertarget{ref-musgrave2020metric}{}%
Musgrave, K., Belongie, S., \& Lim, S.-N. (2020). A metric learning
reality check. \emph{European Conference on Computer Vision}, 681--699.
Springer.

\leavevmode\hypertarget{ref-newton_facebook_2020}{}%
Newton, C. (2020). Facebook will pay \$52 million in settlement with
moderators who developed {PTSD} on the job. Retrieved from
\url{https://www.theverge.com/2020/5/12/21255870/facebook-content-moderator-settlement-scola-ptsd-mental-health}

\leavevmode\hypertarget{ref-noauthor_reddit_nodate}{}%
Reddit in 2020. (2020). Retrieved from
\url{https://www.reddit.com/r/blog/comments/k967mm/reddit_in_2020/}

\leavevmode\hypertarget{ref-roberts2014behind}{}%
Roberts, S. T. (2014). \emph{Behind the screen: The hidden digital labor
of commercial content moderation}. University of Illinois at
Urbana-Champaign.

\leavevmode\hypertarget{ref-roberts2016commercial}{}%
Roberts, S. T. (2016). \emph{Commercial content moderation: Digital
laborers' dirty work}.

\leavevmode\hypertarget{ref-rosa2019automatic}{}%
Rosa, H., Pereira, N., Ribeiro, R., Ferreira, P. C., Carvalho, J. P.,
Oliveira, S., \ldots{} Trancoso, I. (2019). Automatic cyberbullying
detection: A systematic review. \emph{Computers in Human Behavior},
\emph{93}, 333--345.

\leavevmode\hypertarget{ref-rottger2020hatecheck}{}%
Röttger, P., Vidgen, B., Nguyen, D., Waseem, Z., Margetts, H., \&
Pierrehumbert, J. (2020). Hatecheck: Functional tests for hate speech
detection models. \emph{arXiv Preprint arXiv:2012.15606}.

\leavevmode\hypertarget{ref-schmidt2017survey}{}%
Schmidt, A., \& Wiegand, M. (2017). A survey on hate speech detection
using natural language processing. \emph{Proceedings of the Fifth
International Workshop on Natural Language Processing for Social Media},
1--10.

\leavevmode\hypertarget{ref-sejnowski2020unreasonable}{}%
Sejnowski, T. J. (2020). The unreasonable effectiveness of deep learning
in artificial intelligence. \emph{Proceedings of the National Academy of
Sciences}, \emph{117}(48), 30033--30038.

\leavevmode\hypertarget{ref-swamy2019studying}{}%
Swamy, S. D., Jamatia, A., \& Gambäck, B. (2019). Studying
generalisability across abusive language detection datasets.
\emph{Proceedings of the 23rd Conference on Computational Natural
Language Learning (CoNLL)}, 940--950.

\leavevmode\hypertarget{ref-wachs2019associations}{}%
Wachs, S., Wright, M. F., Sittichai, R., Singh, R., Biswal, R., Kim, E.,
\ldots{} others. (2019). Associations between witnessing and
perpetrating online hate in eight countries: The buffering effects of
problem-focused coping. \emph{International Journal of Environmental
Research and Public Health}, \emph{16}(20), 3992.

\leavevmode\hypertarget{ref-wu2019errudite}{}%
Wu, T., Ribeiro, M. T., Heer, J., \& Weld, D. S. (2019). Errudite:
Scalable, reproducible, and testable error analysis. \emph{Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics}, 747--763.

\leavevmode\hypertarget{ref-yin2021towards}{}%
Yin, W., \& Zubiaga, A. (2021). Towards generalisable hate speech
detection: A review on obstacles and solutions. \emph{PeerJ Computer
Science}, \emph{7}, e598.

\end{CSLReferences}

\end{document}
