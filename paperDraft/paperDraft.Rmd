---
title: "Nesta attacks study"
author: "Patrycja Tempska and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: yes
    df_print: kable
    keep_tex: yes
    includes:
      in_header: Rafal_latex6.sty
  html_document:
    df_print: paged
  word_document: default
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: ../references/attacks.bib
csl: ../references/apa-6th-edition.csl
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = '../')

#libraries used
library(ggplot2)
library(ggthemes)
library(gridExtra)

```



\tableofcontents

# Section Abstract
This article describes an experimental intervention study based in a naturalistic, digital setting (Q&A forum - Reddit), utilizing a collective intelligence approach to content moderation and reduction of the level of verbal aggression among a selected group of Reddit users who regularly attack other community members. Collective Intelligence in this sense means exploring the collaboration between human and machine intelligence to develop solutions to social challenges. Artificial Intelligence was used to detect verbal aggression (personal attacks) and notify human volunteers about attacks. Volunteers after receiving notifications employed interventions based on norm or empathy promotion. We find that only those who were sanctioned with norms-inducing interventions had their personal attacks' user significantly decreased. 

\vspace{1mm}
\footnotesize
```{r,echo=TRUE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
2+2 #use this formatting for chunks
```
\normalsize


# Introduction
Although much effort has been made in order to tackle the problem of verbal aggression and harassment online, looking at various reports and surveys, it remains a common hindrance for people engaging with social media in their everyday lives. The situation got exacerbated amidst the COVID19 pandemic, during which a majority of our social life moved to cyberspace. During this shift, there was an increase in cyberbullying attitudes and perpetration (@barlett2021comparing), 90% increase in public reports of illegal online content\footnote{\href {https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}{https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}}, including 114% increase in non-consensual sharing of intimate images, 30% increase in cyberbullying, as well as 40% of increase in adults reporting online harassment. According to a report conducted by company L1ght \footnote{\href {https://l1ght.com/Toxicity_during_coronavirus_Report-L1ght.pdf}{https://l1ght.com/Toxicity_during_coronavirus_Report-L1ght.pdf}}, hate speech directed towards China and the Chinese went up by 900% on Twitter. Gaming platforms were in the spotlight as well, with a 40% increase in toxicity on Discord. 

But alongside the growing need for even more efficient and proactive moderation, the capacity to execute it did not go hand in hand, forcing companies and policymakers to rethink the current model of moderation processes and workforce. Due to the COVID19 restrictions including social distancing, a lot of those serving the role of moderators had to be sent home\footnote{\href {https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}{https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}} without the ability to work remotely because of the constraints affiliated with restrictive non-disclosure agreements (NDA) among others. Curtailing the moderators' workforce was accompanied by more agency given to algorithms and AI-based moderation. Those changes, as argued by @gerrard2020covid19, can be seen as a serious red flag in terms of safety for all users on online platforms. 

# Automated versus Human-based Moderation
The hindrances and threats that go along with the Artificial Intelligence-based methods for moderation have been widely debated, with the most critical discussions revolving around technology performance (@macavaney2019hate, @schmidt2017survey). State-of-the-art solutions are mostly governed by statistical methods including deep learning and machine learning (@lecun2015deep, @sejnowski2020unreasonable, @jordan2015machine). Their performance is inherently tied to the amount of data being fed to the system and the quality of its annotation. At different stages of the process, from datasets gathering and preparation, annotation to the training or algorithms themselves, biases seem to be omnipresent (@binns2017like, @geva2019we @mehrabi2021survey). Users' of online services are also creative in their strategies to circumvent automated content moderation systems and as shown by @grondahl2018all, current techniques are vulnerable to the most common evasion attacks like word changes (inseting typos and leetspeak), word-boundary changes (inserting or removing whitespace), or word appending (appending common or non-hateful words like "love"). Generalisability of the models - an ability to perform well on datasets coming from sources other than the one used for training are an important shortcoming as well (@yin2021towards, @swamy2019studying, @rosa2019automatic). As shown by @wu2019errudite, @lipton2019troubling, and @musgrave2020metric in practice, creations of models often lacks thorough error analysis and legitimate experimental methodology, which can result in non-reproducibility. This is also connected with a potential lack of thorough understanding of the limitations of the models and spurious conclusions being made to a wider public. Specifically, @@lipton2019troubling distinguishes four dysfunctional patterns occurring in the current research paradigm in the industry and academia alike. First, the inability to draw a clear distinction between speculation and explanation, with the first one often being disguised as the second. Second, inability for successful identification of the sources of empirical gains (whether it was problem formulation, optimization of the heuristics, data-preprocessing, hyperparameter tuning, or perhaps yet another aspect). Third, "mathiness" - the use obscure language and often covering weak argumentation with the alluring but often apparent depth of technical jargon. Last but not least - misuse of language. This includes suggestive definitions without proper explanation of what they mean in the context (e.g. inflating good performance in simple NLP tasks to human-level natural understanding), overloading the papers with technical terminology, or suitcase words (those words that can encompass a variety of meanings, e.g. consciousness).

Yet another obstacle in the process is the lack of gold standard in dataset creation and taxonomies of abusive language being used for instance in the process of annotating different datasets. Frequently people obtain data from various sources and do not follow any universally used instructions when it comes to its annotation, leading to discrepancies between various datasets being tagged within one domain (e.g. hate speech). Lack of expert annotators and proper annotation criteria and instructions are also widespread, with the common practice hiring untrained workers from Mechanical Turk or other crowdsourcing platforms.

Although there are some initiatives developed in response, most notably, functional tests for Hate Speech Detection Models created by @rottger2020hatecheck, or the Online Safety Data Initiative (OSDI) LINK https://onlinesafetydata.blog.gov.uk/about-us/, focused on projects related to improving access to data, standardizing the description of online harms, as well as creating tools and benchmarks for evaluation of technologies focused on safety, much effort must be made before wider adoption of such solutions comes into force.  


#  References {-}

\vspace{-3mm}





