% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  10pt,
  dvipsnames,enabledeprecatedfontcommands]{scrartcl}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Nesta attacks study},
  pdfauthor={Patrycja Tempska and Rafal Urbaniak},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\documentclass{article}

% %packages
 \usepackage{booktabs}

\usepackage{multirow}

\usepackage{graphicx}
\usepackage{longtable}
\usepackage{ragged2e}
\usepackage{etex}
%\usepackage{yfonts}
\usepackage{marvosym}
\usepackage[notextcomp]{kpfonts}
\usepackage{nicefrac}
\newcommand*{\QED}{\hfill \footnotesize {\sc Q.e.d.}}
\usepackage{floatrow}

\usepackage[textsize=footnotesize]{todonotes}
%\linespread{1.5}


\setlength{\parindent}{10pt}
\setlength{\parskip}{1pt}


%language
\usepackage{times}
\usepackage{t1enc}
%\usepackage[utf8x]{inputenc}
%\usepackage[polish]{babel}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx}
\usepackage[scaled=0.88]{helvet}


%AMS
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{geometry}
 \geometry{a4paper,left=35mm,top=20mm,}


%environments
\newtheorem{fact}{Fact}



%abbreviations
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\n}{\neg}
\newcommand{\et}{\wedge}
\newcommand{\jt}{\rightarrow}
\newcommand{\ko}[1]{\forall  #1\,}
\newcommand{\ro}{\leftrightarrow}
\newcommand{\exi}[1]{\exists\, {_{#1}}}
\newcommand{\pr}[1]{\mathsf{P}(#1)}
\newcommand{\cost}{\mathsf{cost}}


\newcommand{\odds}{\mathsf{Odds}}
\newcommand{\ind}{\mathsf{Ind}}
\newcommand{\nf}[2]{\nicefrac{#1\,}{#2}}
\newcommand{\R}[1]{\texttt{#1}}
\newcommand{\prr}[1]{\mbox{$\mathtt{P}_{prior}(#1)$}}
\newcommand{\prp}[1]{\mbox{$\mathtt{P}_{posterior}(#1)$}}



\newtheorem{q}{\color{blue}Question}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}



%technical intermezzo
%---------------------

\newcommand{\intermezzoa}{
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}



	\tiny{\sc Optional Content Starts}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}\nopagebreak 
	}


\newcommand{\intermezzob}{\nopagebreak 
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}

	\tiny{\sc Optional Content Ends}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}
	}
%--------------------






















\newtheorem*{reply*}{Reply}
\usepackage{enumitem}
\newcommand{\question}[1]{\begin{enumerate}[resume,leftmargin=0cm,labelsep=0cm,align=left]
\item #1
\end{enumerate}}

\usepackage{float}

% \setbeamertemplate{blocks}[rounded][shadow=true]
% \setbeamertemplate{itemize items}[ball]
% \AtBeginPart{}
% \AtBeginSection{}
% \AtBeginSubsection{}
% \AtBeginSubsubsection{}
% \setlength{\emergencystretch}{0em}
% \setlength{\parskip}{0pt}






\usepackage[authoryear]{natbib}

%\bibliographystyle{apalike}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Nesta attacks study}
\author{Patrycja Tempska and Rafal Urbaniak}
\date{}

\begin{document}
\maketitle

\tableofcontents

\hypertarget{section-abstract}{%
\section{Section Abstract}\label{section-abstract}}

This article describes an experimental intervention study based in a
naturalistic, digital setting (Q\&A forum - Reddit), utilizing a
collective intelligence approach to content moderation and reduction of
the level of verbal aggression among a selected group of Reddit users
who regularly attack other community members. Collective Intelligence in
this sense means exploring the collaboration between human and machine
intelligence to develop solutions to social challenges. Artificial
Intelligence was used to detect verbal aggression (personal attacks) and
notify human volunteers about attacks. Volunteers after receiving
notifications employed interventions based on norm or empathy promotion.
We find that only those who were sanctioned with norms-inducing
interventions had their personal attacks' user significantly decreased.

\vspace{1mm}
\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\SpecialCharTok{+}\DecValTok{2} \CommentTok{\#use this formatting for chunks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\normalsize

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Although much effort has been made in order to tackle the problem of
verbal aggression and harassment online, looking at various reports and
surveys, it remains a common hindrance for people engaging with social
media in their everyday lives. The situation got exacerbated amidst the
COVID19 pandemic, during which a majority of our social life moved to
cyberspace. During this shift, there was an increase in cyberbullying
attitudes and perpetration (Barlett, Simmers, Roth, \& Gentile (2021)),
90\% increase in public reports of illegal online
content\footnote{\href {https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}{https://www.aspistrategist.org.au/australias-esafety-commissioner-targets-abuse-online-as-covid-19-supercharges-cyberbullying/}},
including 114\% increase in non-consensual sharing of intimate images,
30\% increase in cyberbullying, as well as 40\% of increase in adults
reporting online harassment. According to a report conducted by company
L1ght \textbackslash footnote\{\textbackslash href
\{\url{https://l1ght.com/Toxicity_during_coronavirus_Report-L1ght.pdf}\}\{\url{https://l1ght.com/Toxicity_during_coronavirus_Report-L1ght.pdf}\}\},
hate speech directed towards China and the Chinese went up by 900\% on
Twitter. Gaming platforms were in the spotlight as well, with a 40\%
increase in toxicity on Discord.

But alongside the growing need for even more efficient and proactive
moderation, the capacity to execute it did not go hand in hand, forcing
companies and policymakers to rethink the current model of moderation
processes and workforce. Due to the COVID19 restrictions including
social distancing, a lot of those serving the role of moderators had to
be sent
home\footnote{\href {https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}{https://qz.com/india/1976450/facebook-covid-19-lockdowns-hurt-content-moderation-algorithms/}}
without the ability to work remotely because of the constraints
affiliated with restrictive non-disclosure agreements (NDA) among
others. Curtailing the moderators' workforce was accompanied by more
agency given to algorithms and AI-based moderation. Those changes, as
argued by Gerrard (2020), can be seen as a serious red flag in terms of
safety for all users on online platforms.

\hypertarget{automated-versus-human-based-moderation}{%
\section{Automated versus Human-based
Moderation}\label{automated-versus-human-based-moderation}}

The hindrances and threats that go along with the Artificial
Intelligence-based methods for moderation have been widely debated, with
the most critical discussions revolving around technology performance
(MacAvaney et al. (2019), Schmidt \& Wiegand (2017)). State-of-the-art
solutions are mostly governed by statistical methods including deep
learning and machine learning (LeCun, Bengio, \& Hinton (2015),
Sejnowski (2020), Jordan \& Mitchell (2015)). Their performance is
inherently tied to the amount of data being fed to the system and the
quality of its annotation. At different stages of the process, from
datasets gathering and preparation, annotation to the training or
algorithms themselves, biases seem to be omnipresent (Binns, Veale, Van
Kleek, \& Shadbolt (2017), Geva, Goldberg, \& Berant (2019) Mehrabi,
Morstatter, Saxena, Lerman, \& Galstyan (2021)). Users' of online
services are also creative in their strategies to circumvent automated
content moderation systems and as shown by Gröndahl, Pajola, Juuti,
Conti, \& Asokan (2018), current techniques are vulnerable to the most
common evasion attacks like word changes (inseting typos and leetspeak),
word-boundary changes (inserting or removing whitespace), or word
appending (appending common or non-hateful words like ``love'').
Generalisability of the models - an ability to perform well on datasets
coming from sources other than the one used for training are an
important shortcoming as well (Yin \& Zubiaga (2021), Swamy, Jamatia, \&
Gambäck (2019), Rosa et al. (2019)). As shown by Wu, Ribeiro, Heer, \&
Weld (2019), Lipton \& Steinhardt (2019), and Musgrave, Belongie, \& Lim
(2020) in practice, creations of models often lacks thorough error
analysis and legitimate experimental methodology, which can result in
non-reproducibility. This is also connected with a potential lack of
thorough understanding of the limitations of the models and spurious
conclusions being made to a wider public. Specifically, @Lipton \&
Steinhardt (2019) distinguishes four dysfunctional patterns occurring in
the current research paradigm in the industry and academia alike. First,
the inability to draw a clear distinction between speculation and
explanation, with the first one often being disguised as the second.
Second, inability for successful identification of the sources of
empirical gains (whether it was problem formulation, optimization of the
heuristics, data-preprocessing, hyperparameter tuning, or perhaps yet
another aspect). Third, ``mathiness'' - the use obscure language and
often covering weak argumentation with the alluring but often apparent
depth of technical jargon. Last but not least - misuse of language. This
includes suggestive definitions without proper explanation of what they
mean in the context (e.g.~inflating good performance in simple NLP tasks
to human-level natural understanding), overloading the papers with
technical terminology, or suitcase words (those words that can encompass
a variety of meanings, e.g.~consciousness).

Yet another obstacle in the process is the lack of gold standard in
dataset creation and taxonomies of abusive language being used for
instance in the process of annotating different datasets. Frequently
people obtain data from various sources and do not follow any
universally used instructions when it comes to its annotation, leading
to discrepancies between various datasets being tagged within one domain
(e.g.~hate speech). Lack of expert annotators and proper annotation
criteria and instructions are also widespread, with the common practice
hiring untrained workers from Mechanical Turk or other crowdsourcing
platforms.

Although there are some initiatives developed in response, most notably,
functional tests for Hate Speech Detection Models created by Röttger et
al. (2020), or the Online Safety Data Initiative (OSDI) LINK
\url{https://onlinesafetydata.blog.gov.uk/about-us/}, focused on
projects related to improving access to data, standardizing the
description of online harms, as well as creating tools and benchmarks
for evaluation of technologies focused on safety, much effort must be
made before wider adoption of such solutions comes into force.

At the same time, only automated methods can scan through the massive
amount of content being generated every day on different platforms. On
Facebook, there are more than 3B comments and likes daily
(\url{https://martech.org/facebook-3-2-billion-likes-comments-every-day/}),
500M tweets are sent daily on Twitter
(\url{https://www.oberlo.com/blog/twitter-statistics}), and over 2B
comments made by users of Reddit in 2020
(\url{https://old.reddit.com/r/blog/comments/k967mm/reddit_in_2020/})
which is almost 3M comments made daily. With this amount of content,
it's either impossible or extremely costly to scale the moderation
workforce. One can also have doubts about the ethical aspects of hiring
workers who are often unaware of how this kind of task will affect their
well-being. Being submersed in the cyber-Augean stables takes a toll on
many people. As examined by Roberts (2014) \& Roberts (2016), workers
who are hired for such tasks are often low-status and low-wage,
isolated, and asked to keep what they've seen in secret under
restrictive NDAs. Screening through the reported user-generated content
is connected with exposure to violent and deeply disturbing materials,
with child pornography, murders, or suicides as examples of the most
extreme cases. This can lead to serious psychological damage, like
depression, or PTSD (Roberts (2014)). Some of the employees filed a
lawsuit against Facebook and as a result, the company agreed to pay
\$52M in compensation for mental health issues developed during the job
(\url{https://www.theverge.com/2020/5/12/21255870/facebook-content-moderator-settlement-scola-ptsd-mental-health}).
Taking into consideration that Facebook employs 15K moderators
(\url{https://www.forbes.com/sites/johnkoetsier/2020/06/09/300000-facebook-content-moderation-mistakes-daily-report-says/?sh=6c6bdbbc54d0})
and most likely more are needed to keep up with the growing amount of
content, with the parallel considerations about the negative effects of
content moderation on mental health, a collaboration between humans and
machines in this area seems inevitable. There is yet another aspect here
- what moderators deal with is mostly content reported by humans. And as
shown in various studies and reports, a lot of children, teens or even
adults do not report cyberbullying or harassment online (LINKS:
\url{https://www.ctvnews.ca/canada/as-the-pandemic-forces-us-online-lgbtq2s-teens-deal-with-cyberbullying-1.5430945}
; \url{https://www.adl.org/free-to-play-2020} ;
\url{https://www.ditchthelabel.org/wp-content/uploads/2017/05/InGameAbuse.pdf}).

\hypertarget{pro-active-and-reactive-moderation}{%
\section{Pro-active and reactive
moderation}\label{pro-active-and-reactive-moderation}}

\#Collective Intelligence Approach

\#Experimental Design

\#Results

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\vspace{-3mm}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-barlett2021comparing}{}%
Barlett, C. P., Simmers, M. M., Roth, B., \& Gentile, D. (2021).
Comparing cyberbullying prevalence and process before and during the
COVID-19 pandemic. \emph{The Journal of Social Psychology}, 1--11.

\leavevmode\hypertarget{ref-binns2017like}{}%
Binns, R., Veale, M., Van Kleek, M., \& Shadbolt, N. (2017). Like
trainer, like bot? Inheritance of bias in algorithmic content
moderation. \emph{International Conference on Social Informatics},
405--415. Springer.

\leavevmode\hypertarget{ref-gerrard2020covid19}{}%
Gerrard, Y. (2020). \textless? covid19?\textgreater{} The COVID-19
mental health content moderation conundrum. \emph{Social Media+
Society}, \emph{6}(3), 2056305120948186.

\leavevmode\hypertarget{ref-geva2019we}{}%
Geva, M., Goldberg, Y., \& Berant, J. (2019). Are we modeling the task
or the annotator? An investigation of annotator bias in natural language
understanding datasets. \emph{arXiv Preprint arXiv:1908.07898}.

\leavevmode\hypertarget{ref-grondahl2018all}{}%
Gröndahl, T., Pajola, L., Juuti, M., Conti, M., \& Asokan, N. (2018).
All you need is" love" evading hate speech detection. \emph{Proceedings
of the 11th ACM Workshop on Artificial Intelligence and Security},
2--12.

\leavevmode\hypertarget{ref-jordan2015machine}{}%
Jordan, M. I., \& Mitchell, T. M. (2015). Machine learning: Trends,
perspectives, and prospects. \emph{Science}, \emph{349}(6245), 255--260.

\leavevmode\hypertarget{ref-lecun2015deep}{}%
LeCun, Y., Bengio, Y., \& Hinton, G. (2015). Deep learning.
\emph{Nature}, \emph{521}(7553), 436--444.

\leavevmode\hypertarget{ref-lipton2019troubling}{}%
Lipton, Z. C., \& Steinhardt, J. (2019). Troubling trends in machine
learning scholarship: Some ML papers suffer from flaws that could
mislead the public and stymie future research. \emph{Queue},
\emph{17}(1), 45--77.

\leavevmode\hypertarget{ref-macavaney2019hate}{}%
MacAvaney, S., Yao, H.-R., Yang, E., Russell, K., Goharian, N., \&
Frieder, O. (2019). Hate speech detection: Challenges and solutions.
\emph{PloS One}, \emph{14}(8), e0221152.

\leavevmode\hypertarget{ref-mehrabi2021survey}{}%
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., \& Galstyan, A.
(2021). A survey on bias and fairness in machine learning. \emph{ACM
Computing Surveys (CSUR)}, \emph{54}(6), 1--35.

\leavevmode\hypertarget{ref-musgrave2020metric}{}%
Musgrave, K., Belongie, S., \& Lim, S.-N. (2020). A metric learning
reality check. \emph{European Conference on Computer Vision}, 681--699.
Springer.

\leavevmode\hypertarget{ref-roberts2014behind}{}%
Roberts, S. T. (2014). \emph{Behind the screen: The hidden digital labor
of commercial content moderation}. University of Illinois at
Urbana-Champaign.

\leavevmode\hypertarget{ref-roberts2016commercial}{}%
Roberts, S. T. (2016). \emph{Commercial content moderation: Digital
laborers' dirty work}.

\leavevmode\hypertarget{ref-rosa2019automatic}{}%
Rosa, H., Pereira, N., Ribeiro, R., Ferreira, P. C., Carvalho, J. P.,
Oliveira, S., \ldots{} Trancoso, I. (2019). Automatic cyberbullying
detection: A systematic review. \emph{Computers in Human Behavior},
\emph{93}, 333--345.

\leavevmode\hypertarget{ref-rottger2020hatecheck}{}%
Röttger, P., Vidgen, B., Nguyen, D., Waseem, Z., Margetts, H., \&
Pierrehumbert, J. (2020). Hatecheck: Functional tests for hate speech
detection models. \emph{arXiv Preprint arXiv:2012.15606}.

\leavevmode\hypertarget{ref-schmidt2017survey}{}%
Schmidt, A., \& Wiegand, M. (2017). A survey on hate speech detection
using natural language processing. \emph{Proceedings of the Fifth
International Workshop on Natural Language Processing for Social Media},
1--10.

\leavevmode\hypertarget{ref-sejnowski2020unreasonable}{}%
Sejnowski, T. J. (2020). The unreasonable effectiveness of deep learning
in artificial intelligence. \emph{Proceedings of the National Academy of
Sciences}, \emph{117}(48), 30033--30038.

\leavevmode\hypertarget{ref-swamy2019studying}{}%
Swamy, S. D., Jamatia, A., \& Gambäck, B. (2019). Studying
generalisability across abusive language detection datasets.
\emph{Proceedings of the 23rd Conference on Computational Natural
Language Learning (CoNLL)}, 940--950.

\leavevmode\hypertarget{ref-wu2019errudite}{}%
Wu, T., Ribeiro, M. T., Heer, J., \& Weld, D. S. (2019). Errudite:
Scalable, reproducible, and testable error analysis. \emph{Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics}, 747--763.

\leavevmode\hypertarget{ref-yin2021towards}{}%
Yin, W., \& Zubiaga, A. (2021). Towards generalisable hate speech
detection: A review on obstacles and solutions. \emph{PeerJ Computer
Science}, \emph{7}, e598.

\end{CSLReferences}

\end{document}
